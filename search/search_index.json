{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The HEAL Platform \u00b6 The HEAL Platform is a cloud-based and multifunctional web interface that provides a secure environment for discovery and analysis of NIH HEAL results and data. It is designed to serve users with a variety of objectives, backgrounds, and specialties. The HEAL Platform represents a dynamic Data Ecosystem that aggregates and presents data from multiple resources to make data discovery and access easy for users. The platform provides a way to search and query over study metadata and diverse data types, generated by different projects and organizations and stored across multiple secure repositories. The HEAL Platform also offers a secure and cost-effective cloud-computing environment for data analysis, empowering collaborative research and development of new analytical tools. New workflows and results of analyses can be shared with the HEAL community to enable collaborative, high-impact publications that address the opioid crisis. The HEAL Platform is powered by the open-source software \u201cGen3\u201d . Gen3 was created by and is actively developed at the University of Chicago\u2019s Center for Translational Data Science (CTDS) with the aim of creating interoperable cloud-based data resources for the scientific research community. Powered by Watch the introduction video to the HEAL Platform below If your Browser does not support watching this video, here's a link to the video instead.","title":"Home"},{"location":"#the-heal-platform","text":"The HEAL Platform is a cloud-based and multifunctional web interface that provides a secure environment for discovery and analysis of NIH HEAL results and data. It is designed to serve users with a variety of objectives, backgrounds, and specialties. The HEAL Platform represents a dynamic Data Ecosystem that aggregates and presents data from multiple resources to make data discovery and access easy for users. The platform provides a way to search and query over study metadata and diverse data types, generated by different projects and organizations and stored across multiple secure repositories. The HEAL Platform also offers a secure and cost-effective cloud-computing environment for data analysis, empowering collaborative research and development of new analytical tools. New workflows and results of analyses can be shared with the HEAL community to enable collaborative, high-impact publications that address the opioid crisis. The HEAL Platform is powered by the open-source software \u201cGen3\u201d . Gen3 was created by and is actively developed at the University of Chicago\u2019s Center for Translational Data Science (CTDS) with the aim of creating interoperable cloud-based data resources for the scientific research community. Powered by Watch the introduction video to the HEAL Platform below If your Browser does not support watching this video, here's a link to the video instead.","title":"The HEAL Platform"},{"location":"cedar-getting-started/","text":"CEDAR form: Getting Started \u00b6 Getting Started: The least you need to know \u00b6 Remember to Save! You can collaborate to complete the form by sharing it Make sure you know which fields do and don't apply to your study --> Read the \"Sections: Overview & Context\" page (coming soon) before starting the form Detailed coverage for each field and response options are available in the \"CEDAR Fields - Details\" section (coming soon) Check out the FAQs document (coming soon) for questions and answers you may also be interested in! Read below for tech tips on finding, using, and sharing the CEDAR form. Accessing and Using the CEDAR form \u00b6 Where to find it \u00b6 Log in to cedar.metadatacenter.org . Click \"Shared with me\" in the left panel (#1). Hover over the tiles to see the full study names and find the right form (#2). Note: The file will not be in any of the folders (red X). I can't find my form! \u00b6 Step 1: Are you the person who registered the study? Yes, I registered the study: Skip to Step 2. No, someone else registered the study: The CEDAR form is initially available to whoever registered the study. However, it can be shared. The person who registered the study can share it with you by following the instructions in this section . If that is not a solution, request that the form be shared with you at heal-support@gen3.org (please CC a PI on your email if you are not a PI). Step 2: On the Shared with Me page (#1), turn off any filters by clicking Reset All (#2) if you see it there. Your study form may appear after resetting filters to the default. If neither of these steps resolves your access to the CEDAR form, please reach out to heal-support@gen3.org for assistance. Collapse and expand sections \u00b6 By default, the CEDAR forms open in \"expanded\" format, with all metadata sections and fields visible. Section headings are in green text, and metadata fields are in black. You can collapse or expand a section by clicking on the green section heading. Consider \"collapsing\" all sections when you open the form, and expanding them individually as you move through the form. Save \u00b6 The form does NOT autosave. Please be sure to save your entries by scrolling to the bottom of your form and clicking the green \u201cSave\u201d button at bottom right of the form. Saving will not close the page or redirect you. Save often while completing the form so you do not lose any entered metadata. Consider saving each time you complete a form section (or more often). Be aware that the window will automatically log out after a certain period of inactivity, and it will NOT auto-save. Be sure to save before you switch tasks or step away from your computer. Sharing with collaborators \u00b6 The person who completed registration of the study on the HEAL Data Platform will be given access to the study's CEDAR form; however, multiple team members can collaborate on a form. The person who registered the study will be able to share access with other team members. To provide team members access to a CEDAR form: First: Each team member needs a CEDAR account. To create an account, visit cedar.metadatacenter.org . Click \"Register\" below the Password line. Enter your first name, last name, a functional email address (that you can click on a verification email for), and a password. The person who registered the study can share the study's CEDAR form with other team members. Find the CEDAR form by logging in to cedar.metadatacenter.org/ , then clicking \"Shared with me\" (#1). Hover over the tiles to see the full study names and find the right form. Click on the vertical three dots in the right corner of the tile for the CEDAR form (#2), then click on \"Share\" (#3). In the pop-up window, under \"With People,\" start typing the collaborator's name (#4). Select their name from the list below (#5) Use the dropdown on the right (#6) to select \"can write\" (#7) if you want the collaborator to make edits, or \"can read\" for collaborators who will only review, not edit. Then click the OK button (#8). You will see the collaborator's name appear on the right side of the window (red circle). Click Done (#9) to close the window and save changes. You can remove access from collaborators who have left the team by clicking on the X on the right side of the window. Help text & tooltips \u00b6 Help text for each field is available while interacting with the field. Once you click into a field, a \"?\" icon is available in the upper right of the field area. Mouse over the \"?\" icon to view help text for that field. How to select or remove a response for a field \u00b6 Most fields on the CEDAR form have a drop-down panel of options. Click in the field to interact with the response options. You may then click to open the drop-down list of responses. Click on a response to select it for the CEDAR field. If the response field says \"Select options\" instead of \"Select option,\" you may choose multiple responses for a field; simply click all options in the drop-down that apply. To remove a response that has been selected, click in the dropdown to select a new response. Or, for fields that allow multiple responses, click the X to the right of the Jump to Sections: Overview & Context page (coming soon) Jump to CEDAR Fields - Details page (coming soon) Jump to FAQs (coming soon)","title":"CEDAR form: Getting Started"},{"location":"cedar-getting-started/#cedar-form-getting-started","text":"","title":"CEDAR form: Getting Started"},{"location":"cedar-getting-started/#getting-started-the-least-you-need-to-know","text":"Remember to Save! You can collaborate to complete the form by sharing it Make sure you know which fields do and don't apply to your study --> Read the \"Sections: Overview & Context\" page (coming soon) before starting the form Detailed coverage for each field and response options are available in the \"CEDAR Fields - Details\" section (coming soon) Check out the FAQs document (coming soon) for questions and answers you may also be interested in! Read below for tech tips on finding, using, and sharing the CEDAR form.","title":"Getting Started: The least you need to know"},{"location":"cedar-getting-started/#accessing-and-using-the-cedar-form","text":"","title":"Accessing and Using the CEDAR form"},{"location":"cedar-getting-started/#where-to-find-it","text":"Log in to cedar.metadatacenter.org . Click \"Shared with me\" in the left panel (#1). Hover over the tiles to see the full study names and find the right form (#2). Note: The file will not be in any of the folders (red X).","title":"Where to find it"},{"location":"cedar-getting-started/#i-cant-find-my-form","text":"Step 1: Are you the person who registered the study? Yes, I registered the study: Skip to Step 2. No, someone else registered the study: The CEDAR form is initially available to whoever registered the study. However, it can be shared. The person who registered the study can share it with you by following the instructions in this section . If that is not a solution, request that the form be shared with you at heal-support@gen3.org (please CC a PI on your email if you are not a PI). Step 2: On the Shared with Me page (#1), turn off any filters by clicking Reset All (#2) if you see it there. Your study form may appear after resetting filters to the default. If neither of these steps resolves your access to the CEDAR form, please reach out to heal-support@gen3.org for assistance.","title":"I can't find my form!"},{"location":"cedar-getting-started/#collapse-and-expand-sections","text":"By default, the CEDAR forms open in \"expanded\" format, with all metadata sections and fields visible. Section headings are in green text, and metadata fields are in black. You can collapse or expand a section by clicking on the green section heading. Consider \"collapsing\" all sections when you open the form, and expanding them individually as you move through the form.","title":"Collapse and expand sections"},{"location":"cedar-getting-started/#save","text":"The form does NOT autosave. Please be sure to save your entries by scrolling to the bottom of your form and clicking the green \u201cSave\u201d button at bottom right of the form. Saving will not close the page or redirect you. Save often while completing the form so you do not lose any entered metadata. Consider saving each time you complete a form section (or more often). Be aware that the window will automatically log out after a certain period of inactivity, and it will NOT auto-save. Be sure to save before you switch tasks or step away from your computer.","title":"Save"},{"location":"cedar-getting-started/#sharing-with-collaborators","text":"The person who completed registration of the study on the HEAL Data Platform will be given access to the study's CEDAR form; however, multiple team members can collaborate on a form. The person who registered the study will be able to share access with other team members. To provide team members access to a CEDAR form: First: Each team member needs a CEDAR account. To create an account, visit cedar.metadatacenter.org . Click \"Register\" below the Password line. Enter your first name, last name, a functional email address (that you can click on a verification email for), and a password. The person who registered the study can share the study's CEDAR form with other team members. Find the CEDAR form by logging in to cedar.metadatacenter.org/ , then clicking \"Shared with me\" (#1). Hover over the tiles to see the full study names and find the right form. Click on the vertical three dots in the right corner of the tile for the CEDAR form (#2), then click on \"Share\" (#3). In the pop-up window, under \"With People,\" start typing the collaborator's name (#4). Select their name from the list below (#5) Use the dropdown on the right (#6) to select \"can write\" (#7) if you want the collaborator to make edits, or \"can read\" for collaborators who will only review, not edit. Then click the OK button (#8). You will see the collaborator's name appear on the right side of the window (red circle). Click Done (#9) to close the window and save changes. You can remove access from collaborators who have left the team by clicking on the X on the right side of the window.","title":"Sharing with collaborators"},{"location":"cedar-getting-started/#help-text-tooltips","text":"Help text for each field is available while interacting with the field. Once you click into a field, a \"?\" icon is available in the upper right of the field area. Mouse over the \"?\" icon to view help text for that field.","title":"Help text &amp; tooltips"},{"location":"cedar-getting-started/#how-to-select-or-remove-a-response-for-a-field","text":"Most fields on the CEDAR form have a drop-down panel of options. Click in the field to interact with the response options. You may then click to open the drop-down list of responses. Click on a response to select it for the CEDAR field. If the response field says \"Select options\" instead of \"Select option,\" you may choose multiple responses for a field; simply click all options in the drop-down that apply. To remove a response that has been selected, click in the dropdown to select a new response. Or, for fields that allow multiple responses, click the X to the right of the Jump to Sections: Overview & Context page (coming soon) Jump to CEDAR Fields - Details page (coming soon) Jump to FAQs (coming soon)","title":"How to select or remove a response for a field"},{"location":"contact/","text":"Contact \u00b6 Need help? Please contact our help desk . Powered by","title":"Contact"},{"location":"contact/#contact","text":"Need help? Please contact our help desk . Powered by","title":"Contact"},{"location":"data_mgmt_and_repos/","text":"Data Management and Repositories \u00b6 To help HEAL investigators in the repository selection process, the HEAL Data Stewards at RENCI/RTI have developed Repository Selection Criteria and identified a number of recommended repositories. A HEAL-compliant repository is a FAIR data repository that is NIH-supported and ideally has an API for metadata and data permissions calls. The following is a non-exhaustive selection of repositories and resources currently made accessible through or leveraged by the Heal Platform: Representation of Data Resources on the HEAL Data Platform. Studies from different Data Resources can be filtered and selected on the Discovery Page in the Study Filters section (top panel): Studies can be filtered by Data Resource using their respective tags. For more information from NIH regarding public access and data sharing, visit https://heal.nih.gov/about/public-access-data .","title":"Data Management and Repositories"},{"location":"data_mgmt_and_repos/#data-management-and-repositories","text":"To help HEAL investigators in the repository selection process, the HEAL Data Stewards at RENCI/RTI have developed Repository Selection Criteria and identified a number of recommended repositories. A HEAL-compliant repository is a FAIR data repository that is NIH-supported and ideally has an API for metadata and data permissions calls. The following is a non-exhaustive selection of repositories and resources currently made accessible through or leveraged by the Heal Platform: Representation of Data Resources on the HEAL Data Platform. Studies from different Data Resources can be filtered and selected on the Discovery Page in the Study Filters section (top panel): Studies can be filtered by Data Resource using their respective tags. For more information from NIH regarding public access and data sharing, visit https://heal.nih.gov/about/public-access-data .","title":"Data Management and Repositories"},{"location":"data_sharing_guidance/","text":"Packaging Your Data for Success \u00b6 Introduction \u00b6 This documentation describes general principles for sharing data with the research community in a manner consistent with NIH's 2023 Data Management and Sharing Policy, and with additional HEAL program requirements 1 . These principles should apply to nearly all HEAL-compliant repositories, some of which impose their own specific requirements for how these principles are to be implemented. Why align with requirements to manage and share your data?: NIH and HEAL requirements maximize data usability by making them Findable, Accessible, Interoperable, Reusable (FAIR). 2 Following good data sharing practices can make your lab more efficient by helping you to organize and document your data, reducing the chance of errors, and making individual and collaborative work with colleagues easy and organized; and Identifying and learning to use the latest, appropriate technologies can reduce the time and effort required to manage your data. To help you get started, we have identified minimal requirements and additional best practices to increase the scientific and analytic value of your datasets. Data Packaging: What to Share and When \u00b6 The 2023 NIH policy defines scientific data that must be shared as \"the recorded factual material commonly accepted in the scientific community as of sufficient quality to validate and replicate research findings.\" This definition ties data sharing directly to your own analytic work (i.e., the data you share should permit someone to replicate your own published results) and includes not only data but other materials necessary for replication such as analysis code. Data repositories define open access data to be data which may be accessed or downloaded directly without going through a request and approval process. In contrast, controlled access data require that users submit a request which is then reviewed prior to granting approval (e.g., by a data access committee). In cases where a complete dataset requires controlled access to protect confidentiality, we encourage whenever possible creating a second version of the dataset containing only a subset of the data that can be made openly accessible. It is well-established that openly accessible datasets are used much more frequently than controlled access datasets. Note A third category is registered access , which requires a user to register with a repository before downloading or accessing a dataset. This requirement is imposed by the repository (e.g., to track dataset usage), and does not typically represent a major impediment to data use. When packaging your dataset, it can be helpful to keep your data sharing goal in mind. Three common goals are: Replicating published analyses : Sharing just those data needed to replicate findings in a specific published work (e.g., paper, poster, report), which often includes only a subset of the data generated over the course of a study but also requires sharing derived variables or the code for generating them. Data archiving : Sharing an entire dataset generated over the course of a study to permit secondary users to plan and perform their own, independent analyses. Unlike the case above, this requires less attention to derived variables but greater attention to the steps required to protect confidentiality while at the same time retaining as much scientific and analytic value as possible. Participating in a collaborative project : Submitting data from one site or study intended primarily to be analyzed and/or archived together with similar data from other sites or studies (e.g., as part of a planned meta- or mega-analysis or for archiving with a domain-specific repository). This typically requires preparing a dataset that meets very specific requirements, and may require harmonization. While each of these goals has somewhat different needs and requirements, there are several general principles that apply across all of them. These are described below, organized into a set of minimal requirements followed by a list of additional best practices. We also provide several tips that you may find helpful when preparing and packaging your data for sharing. Required items \u00b6 These items are required to enable others to use your shared data. Study-level metadata : A human-readable file (e.g., a README file) identifying the study or project that generated the dataset and describing the terms (including any restrictions) governing use of the data. File-level metadata : A file manifest identifying each data file included together with a brief description of what it contains and any other information needed to use and interpret the data (e.g., what measurement techniques and protocol(s), including software names and versions, were used to generate the files). Variable-level metadata : For tabular data (e.g., one row for each observation with each column representing a different variable), one or more files describing each all variable included in the dataset including the variable name,title and/or description, and measurement units or possible values (e.g., possible responses for a patient reported outcome). Data file(s) : The file(s) containing the shared data; these may contain raw, preprocessed data, post-processed or curated data, or files prepared for analysis Example of a high-quality dataset accompanied by sufficient information to access and replicate published findings: NIDA-CTN-0095A2: Reducing Stigma toward People with Opioid Use Disorder among Primary Care Clinicians at NIDA Data Share Best Practices - A Little More Goes a Long Way \u00b6 The following additional steps can add substantial scientific and analytic value to a shared dataset: Include persistent identifiers (e.g., DOIs) to publication(s) generated from the data; this not only aids in replication but can also provide additional documentation for the data with little or no extra work. Include additional variable-level metadata, such as links to the source of specific items (e.g., Common Data Element (CDE) repository, NIH's PhenX Toolkit, or a specific measurement instrument). Include protocol(s) for executing the study, including procedures for collecting, processing (including QC, cleaning, and curation), and managing the data. Include code that may be executed to translate raw file(s) together with corresponding variable-level metadata (when applicable) into commonly used analytic formats such as Stata, R, SAS, Pandas/Python, etc. Include code to perform specific data transformations (e.g., generate derived variables or reorganize data to facilitate a specific analysis). Include pipelines to validate assertions about the data such as skip patterns (e.g., instances where a specific item is skipped based on responses to a previous item). Combine data, metadata, and code into a data package \u2014a specially organized folder containing one or more data files and corresponding JSON- or YAML-formatted metadata files. Data packages make it easy to validate data, to transform data, to read data into analytic software packages, and to share data. Helpful Tips \u00b6 A FAIR data package contains both the data files you intend to share and sufficient information for secondary users to understand and use those data files. As noted above, this additional information may include protocols, data collection instruments, code for manipulating or analyzing the data, discipline-specific metadata files describing the data, and additional documentation. Recommended file formats \u00b6 Data files should be in a non-proprietary, commonly-used format such as CSV files with commas or tabs as delimiters and a header row containing column (i.e., variable) names. Nearly all software for data collection and/or data management is capable of exporting to CSV format, and this ensures that the data may be read by the widest possible range of software, both now and in the future. Data representing responses from a fixed set of choices (e.g., Yes/No items or individual items from a Likert scale) may be recorded as text labels (e.g., \"Yes\" or \"No\") or as integer codes, where the mapping between integers and labels is provided in an accompanying schema (see Item 2 immediately below). Variable-level metadata (VLMD) should be provided in machine-readable form . The best way to do this is with a JSON- or YAML-format schema, though a data dictionary in CSV format may in some cases also be acceptable. The HEAL Data Ecosystem requires that variable-level metadata (VLMD) schemas include at a minimum the variable name and description, while also strongly encouraging inclusion of the following (for each variable): a title (i.e., a human-readable label for the variable), datatype (e.g., integer, float, date, string, boolean, etc.), and possible responses for each variable (e.g., range, list of choices, or constraints). Examples of valid and invalid VLMD files are available here . Note: All variable-level metadata submitted to the HEAL Data Platform must adhere to this specification, and it is also strongly recommended when submitting data to a repository. Data Curation and Quality Control \u00b6 In addition to the VLMD described above, it is best practice for all data to be accompanied by the following minimal study-level metadata (e.g., in the README file): Name of investigator(s) Funding agency and grant number Usage requirements and/or restrictions, including suggested acknowledgement Reference to corresponding publication, if applicable Similarly, it is best practice for all data files to be accompanied by the following file-level metadata (e.g., in the file manifest): Title Description Version identifier (so a dataset can be clearly identified and distinguished from prior or subsequent versions) Any additional information necessary to read the file (e.g., choice of delimiter, use of quotation marks, encoding) MD5 checksum or equivalent (to verify the file has been transferred accurately and not modified) If a schema is provided, data should be valid when checked programmatically against that schema . A useful way to think about a data dictionary (or more generally, a VLMD schema) is that it represents a set of assertions about a dataset. The validity of these assertions can be checked automatically using available software tools (e.g., Frictionless data management framework for Python ). This allows: 1) data curators to confirm that curation steps are being executed correctly, 2) secondary data users to verify that there have been no changes or modifications made prior to receiving the data, and 3) users to harmonize these data with additional data. For human subjects studies : Investigators engaged in human subjects research are responsible for ensuring that they have consent from research participants and IRB approval to share data and that data are deidentified and shared in a manner consistent with the language used in their consent form(s) and any applicable institutional requirements. In general : All direct identifiers must be removed. This is slightly less restrictive than using the Safe Harbor method, as dates may be preserved (but must be shifted, as described below) as well as geographic units smaller than state (e.g., zip code or census tract). All dates must be shifted to reduce the risk of deductive disclosure. For example, those using REDCap to collect data have the option of shifting dates by a random amount between 0 and 364 days back in time, with a constant shift being used for all dates pertaining to the same individual. Any open-ended responses should be removed; these can be coded into a clearly defined set of categories if it's necessary to preserve some of the information for analysis. Local participant IDs should be replaced with standardized, anonymous identifiers; this breaks any link between local materials and the shared dataset. The resulting dataset should be reviewed by an expert at your institution (e.g., a data curation or archiving expert at your institutional library) to identify and mitigate any additional potential disclosure risks. All missing values should be consistently coded using a clearly defined set of categories such as the following : Don't know (participant reports that they do not know the answer to the question) Refused (participant indicates that they prefer not to answer the question) No answer (self-administered item left blank by participant) Legitimately skipped (item does not apply to this participant and/or timepoint) Not collected (item not administered, either intentionally or unintentionally) Missing in error (datum not available for any other reason) In cases where this information was not recorded, the corresponding value should be left blank (i.e., the empty string). Such information should ideally always be recorded and provided when sharing data, since blank values can make a dataset difficult if not impossible to analyze. Data Sharing Resources \u00b6 HEAL VLMD schema Examples of valid and invalid VLMD Components of a README HEAL-compliant repositories NIH Data Management & Sharing Policy Overview NIH Data Management Resource Curating Research Artifacts to Support Scientific Integrity HEAL Stewards Sensitive Data Guidance Metadata in the HEAL Data Ecosystem References \u00b6 National Institutes of Health (2020). Final NIH Policy for Data Management and Sharing. https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html . \u21a9 Wilkinson, M.D., Dumontier, M., Aalbersberg, Ij.J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L.B., Bourne, P.E., et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3, 160018. https://doi.org/10.1038/sdata.2016.18 . \u21a9","title":"Data Sharing Guidance"},{"location":"data_sharing_guidance/#packaging-your-data-for-success","text":"","title":"Packaging Your Data for Success"},{"location":"data_sharing_guidance/#introduction","text":"This documentation describes general principles for sharing data with the research community in a manner consistent with NIH's 2023 Data Management and Sharing Policy, and with additional HEAL program requirements 1 . These principles should apply to nearly all HEAL-compliant repositories, some of which impose their own specific requirements for how these principles are to be implemented. Why align with requirements to manage and share your data?: NIH and HEAL requirements maximize data usability by making them Findable, Accessible, Interoperable, Reusable (FAIR). 2 Following good data sharing practices can make your lab more efficient by helping you to organize and document your data, reducing the chance of errors, and making individual and collaborative work with colleagues easy and organized; and Identifying and learning to use the latest, appropriate technologies can reduce the time and effort required to manage your data. To help you get started, we have identified minimal requirements and additional best practices to increase the scientific and analytic value of your datasets.","title":"Introduction"},{"location":"data_sharing_guidance/#data-packaging-what-to-share-and-when","text":"The 2023 NIH policy defines scientific data that must be shared as \"the recorded factual material commonly accepted in the scientific community as of sufficient quality to validate and replicate research findings.\" This definition ties data sharing directly to your own analytic work (i.e., the data you share should permit someone to replicate your own published results) and includes not only data but other materials necessary for replication such as analysis code. Data repositories define open access data to be data which may be accessed or downloaded directly without going through a request and approval process. In contrast, controlled access data require that users submit a request which is then reviewed prior to granting approval (e.g., by a data access committee). In cases where a complete dataset requires controlled access to protect confidentiality, we encourage whenever possible creating a second version of the dataset containing only a subset of the data that can be made openly accessible. It is well-established that openly accessible datasets are used much more frequently than controlled access datasets. Note A third category is registered access , which requires a user to register with a repository before downloading or accessing a dataset. This requirement is imposed by the repository (e.g., to track dataset usage), and does not typically represent a major impediment to data use. When packaging your dataset, it can be helpful to keep your data sharing goal in mind. Three common goals are: Replicating published analyses : Sharing just those data needed to replicate findings in a specific published work (e.g., paper, poster, report), which often includes only a subset of the data generated over the course of a study but also requires sharing derived variables or the code for generating them. Data archiving : Sharing an entire dataset generated over the course of a study to permit secondary users to plan and perform their own, independent analyses. Unlike the case above, this requires less attention to derived variables but greater attention to the steps required to protect confidentiality while at the same time retaining as much scientific and analytic value as possible. Participating in a collaborative project : Submitting data from one site or study intended primarily to be analyzed and/or archived together with similar data from other sites or studies (e.g., as part of a planned meta- or mega-analysis or for archiving with a domain-specific repository). This typically requires preparing a dataset that meets very specific requirements, and may require harmonization. While each of these goals has somewhat different needs and requirements, there are several general principles that apply across all of them. These are described below, organized into a set of minimal requirements followed by a list of additional best practices. We also provide several tips that you may find helpful when preparing and packaging your data for sharing.","title":"Data Packaging: What to Share and When"},{"location":"data_sharing_guidance/#required-items","text":"These items are required to enable others to use your shared data. Study-level metadata : A human-readable file (e.g., a README file) identifying the study or project that generated the dataset and describing the terms (including any restrictions) governing use of the data. File-level metadata : A file manifest identifying each data file included together with a brief description of what it contains and any other information needed to use and interpret the data (e.g., what measurement techniques and protocol(s), including software names and versions, were used to generate the files). Variable-level metadata : For tabular data (e.g., one row for each observation with each column representing a different variable), one or more files describing each all variable included in the dataset including the variable name,title and/or description, and measurement units or possible values (e.g., possible responses for a patient reported outcome). Data file(s) : The file(s) containing the shared data; these may contain raw, preprocessed data, post-processed or curated data, or files prepared for analysis Example of a high-quality dataset accompanied by sufficient information to access and replicate published findings: NIDA-CTN-0095A2: Reducing Stigma toward People with Opioid Use Disorder among Primary Care Clinicians at NIDA Data Share","title":"Required items"},{"location":"data_sharing_guidance/#best-practices-a-little-more-goes-a-long-way","text":"The following additional steps can add substantial scientific and analytic value to a shared dataset: Include persistent identifiers (e.g., DOIs) to publication(s) generated from the data; this not only aids in replication but can also provide additional documentation for the data with little or no extra work. Include additional variable-level metadata, such as links to the source of specific items (e.g., Common Data Element (CDE) repository, NIH's PhenX Toolkit, or a specific measurement instrument). Include protocol(s) for executing the study, including procedures for collecting, processing (including QC, cleaning, and curation), and managing the data. Include code that may be executed to translate raw file(s) together with corresponding variable-level metadata (when applicable) into commonly used analytic formats such as Stata, R, SAS, Pandas/Python, etc. Include code to perform specific data transformations (e.g., generate derived variables or reorganize data to facilitate a specific analysis). Include pipelines to validate assertions about the data such as skip patterns (e.g., instances where a specific item is skipped based on responses to a previous item). Combine data, metadata, and code into a data package \u2014a specially organized folder containing one or more data files and corresponding JSON- or YAML-formatted metadata files. Data packages make it easy to validate data, to transform data, to read data into analytic software packages, and to share data.","title":"Best Practices - A Little More Goes a Long Way"},{"location":"data_sharing_guidance/#helpful-tips","text":"A FAIR data package contains both the data files you intend to share and sufficient information for secondary users to understand and use those data files. As noted above, this additional information may include protocols, data collection instruments, code for manipulating or analyzing the data, discipline-specific metadata files describing the data, and additional documentation.","title":"Helpful Tips"},{"location":"data_sharing_guidance/#recommended-file-formats","text":"Data files should be in a non-proprietary, commonly-used format such as CSV files with commas or tabs as delimiters and a header row containing column (i.e., variable) names. Nearly all software for data collection and/or data management is capable of exporting to CSV format, and this ensures that the data may be read by the widest possible range of software, both now and in the future. Data representing responses from a fixed set of choices (e.g., Yes/No items or individual items from a Likert scale) may be recorded as text labels (e.g., \"Yes\" or \"No\") or as integer codes, where the mapping between integers and labels is provided in an accompanying schema (see Item 2 immediately below). Variable-level metadata (VLMD) should be provided in machine-readable form . The best way to do this is with a JSON- or YAML-format schema, though a data dictionary in CSV format may in some cases also be acceptable. The HEAL Data Ecosystem requires that variable-level metadata (VLMD) schemas include at a minimum the variable name and description, while also strongly encouraging inclusion of the following (for each variable): a title (i.e., a human-readable label for the variable), datatype (e.g., integer, float, date, string, boolean, etc.), and possible responses for each variable (e.g., range, list of choices, or constraints). Examples of valid and invalid VLMD files are available here . Note: All variable-level metadata submitted to the HEAL Data Platform must adhere to this specification, and it is also strongly recommended when submitting data to a repository.","title":"Recommended file formats"},{"location":"data_sharing_guidance/#data-curation-and-quality-control","text":"In addition to the VLMD described above, it is best practice for all data to be accompanied by the following minimal study-level metadata (e.g., in the README file): Name of investigator(s) Funding agency and grant number Usage requirements and/or restrictions, including suggested acknowledgement Reference to corresponding publication, if applicable Similarly, it is best practice for all data files to be accompanied by the following file-level metadata (e.g., in the file manifest): Title Description Version identifier (so a dataset can be clearly identified and distinguished from prior or subsequent versions) Any additional information necessary to read the file (e.g., choice of delimiter, use of quotation marks, encoding) MD5 checksum or equivalent (to verify the file has been transferred accurately and not modified) If a schema is provided, data should be valid when checked programmatically against that schema . A useful way to think about a data dictionary (or more generally, a VLMD schema) is that it represents a set of assertions about a dataset. The validity of these assertions can be checked automatically using available software tools (e.g., Frictionless data management framework for Python ). This allows: 1) data curators to confirm that curation steps are being executed correctly, 2) secondary data users to verify that there have been no changes or modifications made prior to receiving the data, and 3) users to harmonize these data with additional data. For human subjects studies : Investigators engaged in human subjects research are responsible for ensuring that they have consent from research participants and IRB approval to share data and that data are deidentified and shared in a manner consistent with the language used in their consent form(s) and any applicable institutional requirements. In general : All direct identifiers must be removed. This is slightly less restrictive than using the Safe Harbor method, as dates may be preserved (but must be shifted, as described below) as well as geographic units smaller than state (e.g., zip code or census tract). All dates must be shifted to reduce the risk of deductive disclosure. For example, those using REDCap to collect data have the option of shifting dates by a random amount between 0 and 364 days back in time, with a constant shift being used for all dates pertaining to the same individual. Any open-ended responses should be removed; these can be coded into a clearly defined set of categories if it's necessary to preserve some of the information for analysis. Local participant IDs should be replaced with standardized, anonymous identifiers; this breaks any link between local materials and the shared dataset. The resulting dataset should be reviewed by an expert at your institution (e.g., a data curation or archiving expert at your institutional library) to identify and mitigate any additional potential disclosure risks. All missing values should be consistently coded using a clearly defined set of categories such as the following : Don't know (participant reports that they do not know the answer to the question) Refused (participant indicates that they prefer not to answer the question) No answer (self-administered item left blank by participant) Legitimately skipped (item does not apply to this participant and/or timepoint) Not collected (item not administered, either intentionally or unintentionally) Missing in error (datum not available for any other reason) In cases where this information was not recorded, the corresponding value should be left blank (i.e., the empty string). Such information should ideally always be recorded and provided when sharing data, since blank values can make a dataset difficult if not impossible to analyze.","title":"Data Curation and Quality Control"},{"location":"data_sharing_guidance/#data-sharing-resources","text":"HEAL VLMD schema Examples of valid and invalid VLMD Components of a README HEAL-compliant repositories NIH Data Management & Sharing Policy Overview NIH Data Management Resource Curating Research Artifacts to Support Scientific Integrity HEAL Stewards Sensitive Data Guidance Metadata in the HEAL Data Ecosystem","title":"Data Sharing Resources"},{"location":"data_sharing_guidance/#references","text":"National Institutes of Health (2020). Final NIH Policy for Data Management and Sharing. https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html . \u21a9 Wilkinson, M.D., Dumontier, M., Aalbersberg, Ij.J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L.B., Bourne, P.E., et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3, 160018. https://doi.org/10.1038/sdata.2016.18 . \u21a9","title":"References"},{"location":"downloading_files/","text":"Data File Downloads \u00b6 Users can download data files associated with a study by downloading the files directly from the Discovery page, or leveraging the CTDS-owned python software development kit (SDK) and the tool \u201cGen3-client\u201d if the file size exceeds 250 MB. Note that current studies that have datasets of more than 250 MB are those with the following project numbers: a) cdcwonder and b) deaarcos1. Note, that accessing data files requires linked access to all FAIR enabled repositories, as described here . A pop-up window will remind users: Users are reminded to link the account to all other FAIR enabled repositories, as described here . Download Data Files from the Discovery Page \u00b6 Users can download data files up to sizes of 250 MB directly from the Discovery Page. Below you find the simple steps to do so. Navigate to the Discovery Page . Link your accounts to FAIR repositories as described here . Find the study of interest by using the search features or the list of accessible studies . Select the clickable box next to the study. Click on \"Download ZIP\", which will initiate the data download. Select the study and click \"Download ZIP\". Users will be prompted with a window that shows the download is being prepared. Please do not navigate away from this page until the download is complete. After clicking \"Download ZIP\", your download is being prepared. Please do not navigate away from this page until the download is complete. Users will be notified once the download is ready. If the download doesn't start automatically, please follow the link prompted. Users will be notified once the download is ready. Save the file(s) by selecting the directory using the prompted window. If the file size exceeds 250 MB, users will be notified to deselect studies to reduce the size or use other tools: Users are advised to use other tools to download the files if the total file size exceeds 250 MB. Please see the next section for a step-by-step guide using these tools . Download Data Files using the Gen3-client \u00b6 In order to download data files above 250 MB, users will need to utilize the Gen3-client command line tool developed by the University of Chicago\u2019s Center for Translational Data Science. The current studies that have datasets of more than 250 MB are those with the following project numbers: a) cdcwonder and b) deaarcos1. Find below a guide to download data files using the Gen3-client: Log in to the HEAL Platform at https://healdata.org/portal/login . Link your accounts to FAIR repositories as described here . Find and select one or multiple studies of interest on the Discovery Page . For multiple studies, select \"Data Availability\" in the top right corner, click \u201cAvailable\u201d, and choose multiple studies. Click on the button \u201cDownload Manifest\". Select a study of interest, then click on the button \u201cDownload Manifest\". Create and download an API key from your Profile Page . Note where you save the API key on your local machine. Create an API key on the profile page. Download the API key as json file and note the directory where the API key was saved for step 6. Download and configure the Gen3-client a. Follow the download instructions of the Gen3-client here . The client can be downloaded here . b. In your terminal, configure your profile using the following command: gen3-client configure --profile = <profile_name> --cred = <credentials.json> --apiendpoint = <api_endpoint_url> ` Mac/Linux: gen3-client configure --profile = demo --cred = ~/Downloads/demo-credentials.json --apiendpoint = https://healdata.org/ Windows: gen3-client configure --profile=demo --cred=C:\\Users\\demo\\Downloads\\demo-credentials.json --apiendpoint=https://healdata.org/ If the command was succesful, you should get the following output: 10:08:20 Profile 'demo' has been configured successfully. If successfully executed, a configuration file will be stored under the directory the user specified under \u201ccred\u201d. For troubleshooting, refer to the instructions found here . c. Download files by using the following command, which references the manifest file name and its location: gen3-client download-multiple --profile = <profile_name> --manifest = <manifest_file> --download-path = <path_for_files> For example: gen3-client download-multiple --profile = demo --manifest = manifest.json --download-path = downloads 2021 /06/03 16 :48:46 Reading manifest... 200 B / 200 B [===================] 100 .00% 0s WARNING: flag \"rename\" was set to false in \"original\" mode, duplicated files under \"downloads/\" will be overwritten Proceed? [ y/n ] : Enter: y Output: 2021 /06/03 16 :48:47 Total number of GUIDs: 1 2021 /06/03 16 :48:47 Preparing file info for each file, please wait... 1 / 1 [============================================] 100 .00% 0s 2021 /06/03 16 :48:47 File info prepared successfully arcos_all_washpost.tsv.gz 6 .41 GiB / 6 .41 GiB [=======================================================] 100 .00% 0s Download Data Files in Workspaces using the Python SDK \u00b6 Users can download data files to the workspaces by leveraging the CTDS-owned python software development kit (SDK). Follow instructions below. Log in to the Data Commons at https://healdata.org/portal/login . Link your accounts to FAIR repositories as described here . Find and select one or multiple studies of interest on the Discovery Page . Select \"Data Availability\" in the top right corner and click on \u201cAvailable\u201d to see all available studies. Select a study and click on \"Open in Workspace\". Select a workspace VM and click \"Launch\". Choose the \"(Generic) Jupyter Notebook with R kernel\" if you are familiar with setting up Python- or R-based Notebooks, or if you just exported one or multiple studies from the Discovery Page and want to start your custom analysis. Choose a VM with the name of the Notebook if you selected the studies relevant to a specific Notebook and want to work on the Notebook in interactive mode. Available workspaces on the HEAL Platform (top). Users need to link accounts from other repositories (bottom; click here to see how ). Find all files under /data/healdata.org/ with the ending \"PLACEHOLDER\". These files can be directly downloaded either in the terminal or in a notebook cell. Click on one file and copy the command and GUID. Open a new terminal under \"New\". Type in the following command to download the file to the terminal: gen3 drs-pull object \"guid\" If you are working in a notebook, type in the following command into a code cell to download the file: !gen3 drs-pull object \"guid\" If you use the R kernel, change the command into system(\"gen3 drs-pull object 'guid'\") Note, that you can also use the manifest.json to download in batches, see below: Download files in batches with a file manifest using the commands shown above. The file(s) should be downloaded and is ready to be worked with in your Notebook. Downloaded files can be found in your home directory.","title":"Data File Downloads"},{"location":"downloading_files/#data-file-downloads","text":"Users can download data files associated with a study by downloading the files directly from the Discovery page, or leveraging the CTDS-owned python software development kit (SDK) and the tool \u201cGen3-client\u201d if the file size exceeds 250 MB. Note that current studies that have datasets of more than 250 MB are those with the following project numbers: a) cdcwonder and b) deaarcos1. Note, that accessing data files requires linked access to all FAIR enabled repositories, as described here . A pop-up window will remind users: Users are reminded to link the account to all other FAIR enabled repositories, as described here .","title":"Data File Downloads"},{"location":"downloading_files/#download-data-files-from-the-discovery-page","text":"Users can download data files up to sizes of 250 MB directly from the Discovery Page. Below you find the simple steps to do so. Navigate to the Discovery Page . Link your accounts to FAIR repositories as described here . Find the study of interest by using the search features or the list of accessible studies . Select the clickable box next to the study. Click on \"Download ZIP\", which will initiate the data download. Select the study and click \"Download ZIP\". Users will be prompted with a window that shows the download is being prepared. Please do not navigate away from this page until the download is complete. After clicking \"Download ZIP\", your download is being prepared. Please do not navigate away from this page until the download is complete. Users will be notified once the download is ready. If the download doesn't start automatically, please follow the link prompted. Users will be notified once the download is ready. Save the file(s) by selecting the directory using the prompted window. If the file size exceeds 250 MB, users will be notified to deselect studies to reduce the size or use other tools: Users are advised to use other tools to download the files if the total file size exceeds 250 MB. Please see the next section for a step-by-step guide using these tools .","title":"Download Data Files from the Discovery Page"},{"location":"downloading_files/#download-data-files-using-the-gen3-client","text":"In order to download data files above 250 MB, users will need to utilize the Gen3-client command line tool developed by the University of Chicago\u2019s Center for Translational Data Science. The current studies that have datasets of more than 250 MB are those with the following project numbers: a) cdcwonder and b) deaarcos1. Find below a guide to download data files using the Gen3-client: Log in to the HEAL Platform at https://healdata.org/portal/login . Link your accounts to FAIR repositories as described here . Find and select one or multiple studies of interest on the Discovery Page . For multiple studies, select \"Data Availability\" in the top right corner, click \u201cAvailable\u201d, and choose multiple studies. Click on the button \u201cDownload Manifest\". Select a study of interest, then click on the button \u201cDownload Manifest\". Create and download an API key from your Profile Page . Note where you save the API key on your local machine. Create an API key on the profile page. Download the API key as json file and note the directory where the API key was saved for step 6. Download and configure the Gen3-client a. Follow the download instructions of the Gen3-client here . The client can be downloaded here . b. In your terminal, configure your profile using the following command: gen3-client configure --profile = <profile_name> --cred = <credentials.json> --apiendpoint = <api_endpoint_url> ` Mac/Linux: gen3-client configure --profile = demo --cred = ~/Downloads/demo-credentials.json --apiendpoint = https://healdata.org/ Windows: gen3-client configure --profile=demo --cred=C:\\Users\\demo\\Downloads\\demo-credentials.json --apiendpoint=https://healdata.org/ If the command was succesful, you should get the following output: 10:08:20 Profile 'demo' has been configured successfully. If successfully executed, a configuration file will be stored under the directory the user specified under \u201ccred\u201d. For troubleshooting, refer to the instructions found here . c. Download files by using the following command, which references the manifest file name and its location: gen3-client download-multiple --profile = <profile_name> --manifest = <manifest_file> --download-path = <path_for_files> For example: gen3-client download-multiple --profile = demo --manifest = manifest.json --download-path = downloads 2021 /06/03 16 :48:46 Reading manifest... 200 B / 200 B [===================] 100 .00% 0s WARNING: flag \"rename\" was set to false in \"original\" mode, duplicated files under \"downloads/\" will be overwritten Proceed? [ y/n ] : Enter: y Output: 2021 /06/03 16 :48:47 Total number of GUIDs: 1 2021 /06/03 16 :48:47 Preparing file info for each file, please wait... 1 / 1 [============================================] 100 .00% 0s 2021 /06/03 16 :48:47 File info prepared successfully arcos_all_washpost.tsv.gz 6 .41 GiB / 6 .41 GiB [=======================================================] 100 .00% 0s","title":"Download Data Files using the Gen3-client"},{"location":"downloading_files/#download-data-files-in-workspaces-using-the-python-sdk","text":"Users can download data files to the workspaces by leveraging the CTDS-owned python software development kit (SDK). Follow instructions below. Log in to the Data Commons at https://healdata.org/portal/login . Link your accounts to FAIR repositories as described here . Find and select one or multiple studies of interest on the Discovery Page . Select \"Data Availability\" in the top right corner and click on \u201cAvailable\u201d to see all available studies. Select a study and click on \"Open in Workspace\". Select a workspace VM and click \"Launch\". Choose the \"(Generic) Jupyter Notebook with R kernel\" if you are familiar with setting up Python- or R-based Notebooks, or if you just exported one or multiple studies from the Discovery Page and want to start your custom analysis. Choose a VM with the name of the Notebook if you selected the studies relevant to a specific Notebook and want to work on the Notebook in interactive mode. Available workspaces on the HEAL Platform (top). Users need to link accounts from other repositories (bottom; click here to see how ). Find all files under /data/healdata.org/ with the ending \"PLACEHOLDER\". These files can be directly downloaded either in the terminal or in a notebook cell. Click on one file and copy the command and GUID. Open a new terminal under \"New\". Type in the following command to download the file to the terminal: gen3 drs-pull object \"guid\" If you are working in a notebook, type in the following command into a code cell to download the file: !gen3 drs-pull object \"guid\" If you use the R kernel, change the command into system(\"gen3 drs-pull object 'guid'\") Note, that you can also use the manifest.json to download in batches, see below: Download files in batches with a file manifest using the commands shown above. The file(s) should be downloaded and is ready to be worked with in your Notebook. Downloaded files can be found in your home directory.","title":"Download Data Files in Workspaces using the Python SDK"},{"location":"faqs/","text":"FAQs \u00b6 What can I do on the HEAL Platform? The HEAL Platform is a cloud-based and multifunctional web interface that provides a secure environment for discovery and analysis of HEAL results and data. Explore all functionalities here . Which datasets are currently open-access? Find all open-access datasets here . Where can I see individual study metadata? Individual study metadata is shown upon clicking on a study entry on the Discovery Page. Click to read the full description here . Do I need to store my data in a particular location to participate in the HEAL platform? You must store your data in a HEAL-compliant repository. A HEAL-compliant repository is a data repository that is NIH-supported and ideally has an API for metadata and data permissions calls. To help HEAL investigators in the repository selection process, the HEAL Data Stewards at RENCI/RTI have developed Repository Selection Criteria and identified a number of recommended repositories. If you have questions regarding costs associated with data storage in HEAL-compliant repositories, please contact the HEAL Data Stewards . How and where can I see if I have access to a study? Clicking on an individual study on the Discovery Page will show in the top right corner if they are accessible to the user. To see all studies users have access to, users should navigate to the Discovery Page and select \u201cData Availability\u201d - Available\u201d. More information here . I do not have access to a specific study, how can I get access? RENCI/RTI (Renaissance Computing Institute) at the University of North Carolina at Chapel Hill establishes and manages data access contacts over all data contributors/sources. To request access to controlled data, users should contact RENCI/RTI at HEALstewards@renci.org. The profile page says I have access but I don\u2019t see it on the Discovery Page. Please contact our help desk . I would like to download data files from a specific study, what do I need to do? Users can download the studies directly from the Discovery Page as described here , or, if the file size exceeds 250 MB, users need to use the Gen3-client command line tool to download data files, as described here . Additional information on the Gen3-client may be found here . How can I get data files into the Workspace from the Discovery Page? Click here to follow the steps on how to get files from the Discovery Page to the Workspace. How do I find a study? Click here to see possible ways to search for a study on the Discovery Page. Can I download files directly from the portal? Yes, you can. Click here to follow the steps to download data files from the Discovery Page. Caution: the download is limited to file sizes of 250 MB. My download is not working. Please check if your file size exceeds 250 MB. If yes, please other tools as described here to download the files. Note that the current studies that have datasets of more than 250 MB are those with the following project numbers: a) cdcwonder and b) deaarcos1. If errors persist, please contact the help desk . My data file download using the Gen3-client gets stuck, where can I get help? For troubleshooting, please see the Gen3-client documentation here . If errors persist, contact our help desk . The Gen3-client shows errors, where can I get help? To check that the client is working and to confirm the client version, type \u2018gen3-client\u2019 in the terminal. Typing \u2018gen3-client help\u2019 will display the help menu. Users must provide the full path of the tool in order for the commands to run, for example, \u2018./gen3-client\u2019 while working from the directory containing the client. For more troubleshooting, please see the Gen3-client documentation here . If errors persist, contact our help desk . I want to work interactively on the Tutorial Notebooks. Show me how. Click here to follow the steps of a demo. What are the current Tutorial Notebooks? Click here to see a list of currently available Notebooks in Python and R. How do I work with these Tutorial Notebooks? Click here to see a guide of how to work with the Tutorial Notebooks. How do I link my account to a FAIR enabled repository? Click here to see how to link your account to a FAIR enabled repository. What do the Workspaces have to offer? Click here to see how to get started and here to see what languages, tools, and environments the workspaces have enabled. I want to report a bug! Please report any errors or bugs to our help desk .","title":"FAQs"},{"location":"faqs/#faqs","text":"What can I do on the HEAL Platform? The HEAL Platform is a cloud-based and multifunctional web interface that provides a secure environment for discovery and analysis of HEAL results and data. Explore all functionalities here . Which datasets are currently open-access? Find all open-access datasets here . Where can I see individual study metadata? Individual study metadata is shown upon clicking on a study entry on the Discovery Page. Click to read the full description here . Do I need to store my data in a particular location to participate in the HEAL platform? You must store your data in a HEAL-compliant repository. A HEAL-compliant repository is a data repository that is NIH-supported and ideally has an API for metadata and data permissions calls. To help HEAL investigators in the repository selection process, the HEAL Data Stewards at RENCI/RTI have developed Repository Selection Criteria and identified a number of recommended repositories. If you have questions regarding costs associated with data storage in HEAL-compliant repositories, please contact the HEAL Data Stewards . How and where can I see if I have access to a study? Clicking on an individual study on the Discovery Page will show in the top right corner if they are accessible to the user. To see all studies users have access to, users should navigate to the Discovery Page and select \u201cData Availability\u201d - Available\u201d. More information here . I do not have access to a specific study, how can I get access? RENCI/RTI (Renaissance Computing Institute) at the University of North Carolina at Chapel Hill establishes and manages data access contacts over all data contributors/sources. To request access to controlled data, users should contact RENCI/RTI at HEALstewards@renci.org. The profile page says I have access but I don\u2019t see it on the Discovery Page. Please contact our help desk . I would like to download data files from a specific study, what do I need to do? Users can download the studies directly from the Discovery Page as described here , or, if the file size exceeds 250 MB, users need to use the Gen3-client command line tool to download data files, as described here . Additional information on the Gen3-client may be found here . How can I get data files into the Workspace from the Discovery Page? Click here to follow the steps on how to get files from the Discovery Page to the Workspace. How do I find a study? Click here to see possible ways to search for a study on the Discovery Page. Can I download files directly from the portal? Yes, you can. Click here to follow the steps to download data files from the Discovery Page. Caution: the download is limited to file sizes of 250 MB. My download is not working. Please check if your file size exceeds 250 MB. If yes, please other tools as described here to download the files. Note that the current studies that have datasets of more than 250 MB are those with the following project numbers: a) cdcwonder and b) deaarcos1. If errors persist, please contact the help desk . My data file download using the Gen3-client gets stuck, where can I get help? For troubleshooting, please see the Gen3-client documentation here . If errors persist, contact our help desk . The Gen3-client shows errors, where can I get help? To check that the client is working and to confirm the client version, type \u2018gen3-client\u2019 in the terminal. Typing \u2018gen3-client help\u2019 will display the help menu. Users must provide the full path of the tool in order for the commands to run, for example, \u2018./gen3-client\u2019 while working from the directory containing the client. For more troubleshooting, please see the Gen3-client documentation here . If errors persist, contact our help desk . I want to work interactively on the Tutorial Notebooks. Show me how. Click here to follow the steps of a demo. What are the current Tutorial Notebooks? Click here to see a list of currently available Notebooks in Python and R. How do I work with these Tutorial Notebooks? Click here to see a guide of how to work with the Tutorial Notebooks. How do I link my account to a FAIR enabled repository? Click here to see how to link your account to a FAIR enabled repository. What do the Workspaces have to offer? Click here to see how to get started and here to see what languages, tools, and environments the workspaces have enabled. I want to report a bug! Please report any errors or bugs to our help desk .","title":"FAQs"},{"location":"hosted_data_types/","text":"Types of Hosted Data \u00b6 As part of the NIH HEAL Initiative, the HEAL Platform aims to transform opioid research into a virtual, annotated, searchable catalog where data from different studies can be analyzed, compared, and combined. The data presented on the HEAL Platform are diverse and include scientific research across multiple disciplines. For example, researchers can use this single platform to find clinical patient care data and public records as well as \u201c-omics\u201d data files with associated sample collection and processing data. The HEAL platform aims to make data more accessible by following the \" FAIR \" principles: F indable Researchers are provided an intuitive interface to search over metadata for HEAL studies and related datasets. Each study and dataset will be assigned a unique, persistent identifier. A ccessible Authenticated users can request and receive access to controlled-access data by data providers. Metadata can be accessed via an open API. I nteroperable Standard metadata vocabularies facilitate discovery and joint analysis of HEAL and related datasets. Data can Data can be easily exported to various workspaces for analysis using a variety of software tools. R eusable Data can be easily reused to facilitate reproducibility of results, development and sharing of new tools, and collaboration between investigators. FAIR data repositories are traditionally a part of a larger institution established for research, data archiving, and, to serve data users of that organization.","title":"Types of Hosted Data"},{"location":"hosted_data_types/#types-of-hosted-data","text":"As part of the NIH HEAL Initiative, the HEAL Platform aims to transform opioid research into a virtual, annotated, searchable catalog where data from different studies can be analyzed, compared, and combined. The data presented on the HEAL Platform are diverse and include scientific research across multiple disciplines. For example, researchers can use this single platform to find clinical patient care data and public records as well as \u201c-omics\u201d data files with associated sample collection and processing data. The HEAL platform aims to make data more accessible by following the \" FAIR \" principles: F indable Researchers are provided an intuitive interface to search over metadata for HEAL studies and related datasets. Each study and dataset will be assigned a unique, persistent identifier. A ccessible Authenticated users can request and receive access to controlled-access data by data providers. Metadata can be accessed via an open API. I nteroperable Standard metadata vocabularies facilitate discovery and joint analysis of HEAL and related datasets. Data can Data can be easily exported to various workspaces for analysis using a variety of software tools. R eusable Data can be easily reused to facilitate reproducibility of results, development and sharing of new tools, and collaboration between investigators. FAIR data repositories are traditionally a part of a larger institution established for research, data archiving, and, to serve data users of that organization.","title":"Types of Hosted Data"},{"location":"logging-in/","text":"Logging In to the Platform \u00b6 You will not need to log in in order to: browse the study metadata on the Discovery Page read the pre-made tutorial notebooks in the \u201cExample Analysis\u201d tab You will need to log in and obtain authorization (access) in order to: register your own study access studies with controlled data perform analyses in workspaces download data files and file manifests run interactive tutorial notebooks in workspaces Start by visiting the login page . Login Page Options \u00b6 Login from Google : You may login using any Google account credentials, or a G-suite enabled institutional email. This option may or may not be available depending on the institution or organization the user is associated with. Users should contact the IT support to verify if this option is available. For staff, students, and faculty of the University of Chicago, more information can be found here . ORCID Login : Users with an ORCID account can log in using their ORCID credentials. InCommon Login : Users can login with a participating institution that is federated by InCommon. Click on \u201cSelect...\u201d to browse and choose the institution. After successfully logging in, your username will appear in the upper right-hand corner of the page.","title":"Logging In"},{"location":"logging-in/#logging-in-to-the-platform","text":"You will not need to log in in order to: browse the study metadata on the Discovery Page read the pre-made tutorial notebooks in the \u201cExample Analysis\u201d tab You will need to log in and obtain authorization (access) in order to: register your own study access studies with controlled data perform analyses in workspaces download data files and file manifests run interactive tutorial notebooks in workspaces Start by visiting the login page .","title":"Logging In to the Platform"},{"location":"logging-in/#login-page-options","text":"Login from Google : You may login using any Google account credentials, or a G-suite enabled institutional email. This option may or may not be available depending on the institution or organization the user is associated with. Users should contact the IT support to verify if this option is available. For staff, students, and faculty of the University of Chicago, more information can be found here . ORCID Login : Users with an ORCID account can log in using their ORCID credentials. InCommon Login : Users can login with a participating institution that is federated by InCommon. Click on \u201cSelect...\u201d to browse and choose the institution. After successfully logging in, your username will appear in the upper right-hand corner of the page.","title":"Login Page Options"},{"location":"platform_discovery_page/","text":"Discovery Page \u00b6 The Discovery Page provides users a venue to search and find studies and datasets displayed in the HEAL Platform. Users can browse through the publicly accessible study-level metadata without requiring authorization. Use text-based search, faceted search, and tags to rapidly and efficiently find relevant studies, discover new datasets across multiple resources, and easily export selected data files to the analysis workspace. The Discovery Page of the HEAL Platform. Browse through datasets and study-level metadata and find studies using tags or the free text search field. Search Features \u00b6 Different features such as free text search bar and tags on the Discovery Page help navigating and refining the search. Free Text Search: Finding studies is made easy using keywords in the free text-based search bar or using tags. The free-text search bar can be used to search for study title, investigator name, or any keyword that is mentioned in the metadata of the study except for Filter options (see #3 below). Data Repository: To filter studies by specific data repositories used by a study, click the Data Repository button (2A), which will reveal a Data Repository field (2B) in which you may select repositories of interest. If you select multiple repositories, it will use an \"OR\" logic to filter studies that have reported any of those repositories. Filters: Click \"Filters\" to expand the options to filter studies by different study metadata tags. Find a range of metadata tags in five categories (Study Type, Data Type, Subject Type, Gender, and Age) to narrow down any search. Selecting multiple tags can work in either an \"AND\" a \"OR\" logic, depending on the selecion at the top of the Filters drawer. Click on \"Reset Filters\" to deselect all filters and start over. Total number of studies: Shows the number of studies the HEAL Platform is currently displaying that match the various search and filter criteria selected. Export Options: Login first to leverage the export options. Select one or multiple studies by checking the checkbox at the beginning of the study row and you can then: 1) download the attached data files (only available for some studies) ; 2) download a file manifest (especially useful for data files whose sizes exceed 250 MB); or 3) export the metadata and data files to secure cloud environment \"Workspaces\" to start your custom data analysis in Python, R, or Stata. Studies: This feature presents all current studies on the HEAL Platform. Click on any study to show useful information about the study (metadata). Data Availability: Filter on available, pending, and not-yet-available datasets. Documentation: This brings you to home page for the documentation (including this page you are currently on). Login Page: Login on the HEAL Platform to leverage all features. Read further here . Study Page \u00b6 You can view the study page by clicking on a study in the search results, which will open a drawer with the study page on the right. The study page defaults to open the Summary tab, but there are 4 tabs in the study page: Summary tab: Includes information about the study, with selected study-level metadata tags listed at the bottom (reflecting metadata reported in the CEDAR form). (Shown in figure above) Data tab: Provides buttons to download variable-level metadata, study-level metadata, a manifest of the metadata files, or download all files. Note: you must be logged in to use some of these features. This tab will also display a link to the data in the repository if it has been provided to the platform. Cite tab: Provides citation information for the study on the platform and for the study data in the repository Related Studies tab: Shows other very-closely-related studies on the platform that share the same NIH serial number ( read more about NIH serial numbers here ). Most studies will not have any related studies listed. Find available Study-level Metadata \u00b6 You can find available study-level metadata for a study on the HEAL Platform from the study page. There are two ways to see the metadata. The study page opens on the Summary tab, which includes information about the study with selected study-level metadata tags listed at the bottom (see figure in Study Page , above). The Data tab on the study page includes a button to download study-level metadata (see Data tab figure, above). Find accessible Datasets \u00b6 On the Discovery page, users can filter for studies that have data available through the platform using the Data Availability filter in the right-most column of the search results. Click on Data Availability (#1), then select only the Available box (#2) to find data that is available and that you have access to through the platform, then click OK (#3). The Discovery Page will automatically update the list of studies that have available datasets. The filter allows you to check more than one availability status, and uses an \"OR\" logic to filter studies. To interpret the different availability status options, please see the next section. Data Availability Options \u00b6 Waiting: \u201cWaiting\u201d is the default data availability status for studies added to the platform. This status means: Data have not yet been submitted to a repository; or, The repository has not yet released the data; or, The study team has not yet provided the link to the data to the HEAL Data Platform. (To submit a data link, see our documentation about this ) Available: \u201cAvailable\u201d means you are now able to access these data via the HEAL Data Platform, by downloading them and/or by importing them in a workspace Request Access: \u201cRequest Access\u201d means that the data has been deposited in a repository and is available. To access the data, you will first need to visit the host repository to request access to it, following the repository\u2019s standard procedures. Once that is complete, authorized users can then bring these data into a HEAL workspace for analysis . Not Available: \u201cNot Available\u201d means that no data will be made available for this study. Select Files on the Discovery Page and bring them to the Workspace \u00b6 The above gif shows the workflow used to select data files from the Discovery Page and bring them into the workspace using Jupyter Notebooks. Log in at https://healdata.org/portal/login . Link your account to all FAIR repositories as described here . Find and select the study by the project number (in this example: 1U2CDA050098-01_a) on the Discovery Page . Find other current open-access studies here . Click the \u201cOpen in Workspace\u201d button in the upper right corner. This step will create a manifest folder which you can find later in the Workspace\u2019s folder \"data/healdata.org\". Select the study and click on \u201cOpen in Workspace\u201d. Choose a workspace flavor and click \"Launch\". This step may take several minutes. Navigate to the /pd/data/healdata.org/ folder and find the placeholder files there. Click on those placeholder files to find instructions how to download the files or see below. From the directory /pd , users can now start a new notebook. Click \u201cNew\u201d (upper right corner) or open a previously saved notebook on the landing drive to load the files into one of the cells, for example by running: ! gen3 drs-pull object \"guid\" import pandas as pd os.chdir('/pd') demo_df = pd.read_csv('pd/demo_file.txt', sep='\\t') demo_df.head() More information for instructions on importing data can be found in the sections \"Download Files to Workspaces\" and \"Workspaces\" . Make sure to terminate the workspace when the work is finished to reduce computational costs. \"pd\" means persistent directory. Saved files outside this directory will be lost. The manifest.json lists metadata of all exported files and can be used to download in batches Note, that all exported data files will be saved in the /pd/data/healdata.org/ folder. Please also note, that the workspace mounts up to a maximum of 5 different manifests while the workspace is running but shows only the latest exported manifest in a newly launched workspace. Terminating the workspace will result in the loss of all but the latests manifest.","title":"Discovery Page"},{"location":"platform_discovery_page/#discovery-page","text":"The Discovery Page provides users a venue to search and find studies and datasets displayed in the HEAL Platform. Users can browse through the publicly accessible study-level metadata without requiring authorization. Use text-based search, faceted search, and tags to rapidly and efficiently find relevant studies, discover new datasets across multiple resources, and easily export selected data files to the analysis workspace. The Discovery Page of the HEAL Platform. Browse through datasets and study-level metadata and find studies using tags or the free text search field.","title":"Discovery Page"},{"location":"platform_discovery_page/#search-features","text":"Different features such as free text search bar and tags on the Discovery Page help navigating and refining the search. Free Text Search: Finding studies is made easy using keywords in the free text-based search bar or using tags. The free-text search bar can be used to search for study title, investigator name, or any keyword that is mentioned in the metadata of the study except for Filter options (see #3 below). Data Repository: To filter studies by specific data repositories used by a study, click the Data Repository button (2A), which will reveal a Data Repository field (2B) in which you may select repositories of interest. If you select multiple repositories, it will use an \"OR\" logic to filter studies that have reported any of those repositories. Filters: Click \"Filters\" to expand the options to filter studies by different study metadata tags. Find a range of metadata tags in five categories (Study Type, Data Type, Subject Type, Gender, and Age) to narrow down any search. Selecting multiple tags can work in either an \"AND\" a \"OR\" logic, depending on the selecion at the top of the Filters drawer. Click on \"Reset Filters\" to deselect all filters and start over. Total number of studies: Shows the number of studies the HEAL Platform is currently displaying that match the various search and filter criteria selected. Export Options: Login first to leverage the export options. Select one or multiple studies by checking the checkbox at the beginning of the study row and you can then: 1) download the attached data files (only available for some studies) ; 2) download a file manifest (especially useful for data files whose sizes exceed 250 MB); or 3) export the metadata and data files to secure cloud environment \"Workspaces\" to start your custom data analysis in Python, R, or Stata. Studies: This feature presents all current studies on the HEAL Platform. Click on any study to show useful information about the study (metadata). Data Availability: Filter on available, pending, and not-yet-available datasets. Documentation: This brings you to home page for the documentation (including this page you are currently on). Login Page: Login on the HEAL Platform to leverage all features. Read further here .","title":"Search Features"},{"location":"platform_discovery_page/#study-page","text":"You can view the study page by clicking on a study in the search results, which will open a drawer with the study page on the right. The study page defaults to open the Summary tab, but there are 4 tabs in the study page: Summary tab: Includes information about the study, with selected study-level metadata tags listed at the bottom (reflecting metadata reported in the CEDAR form). (Shown in figure above) Data tab: Provides buttons to download variable-level metadata, study-level metadata, a manifest of the metadata files, or download all files. Note: you must be logged in to use some of these features. This tab will also display a link to the data in the repository if it has been provided to the platform. Cite tab: Provides citation information for the study on the platform and for the study data in the repository Related Studies tab: Shows other very-closely-related studies on the platform that share the same NIH serial number ( read more about NIH serial numbers here ). Most studies will not have any related studies listed.","title":"Study Page"},{"location":"platform_discovery_page/#find-available-study-level-metadata","text":"You can find available study-level metadata for a study on the HEAL Platform from the study page. There are two ways to see the metadata. The study page opens on the Summary tab, which includes information about the study with selected study-level metadata tags listed at the bottom (see figure in Study Page , above). The Data tab on the study page includes a button to download study-level metadata (see Data tab figure, above).","title":"Find available Study-level Metadata"},{"location":"platform_discovery_page/#find-accessible-datasets","text":"On the Discovery page, users can filter for studies that have data available through the platform using the Data Availability filter in the right-most column of the search results. Click on Data Availability (#1), then select only the Available box (#2) to find data that is available and that you have access to through the platform, then click OK (#3). The Discovery Page will automatically update the list of studies that have available datasets. The filter allows you to check more than one availability status, and uses an \"OR\" logic to filter studies. To interpret the different availability status options, please see the next section.","title":"Find accessible Datasets"},{"location":"platform_discovery_page/#data-availability-options","text":"Waiting: \u201cWaiting\u201d is the default data availability status for studies added to the platform. This status means: Data have not yet been submitted to a repository; or, The repository has not yet released the data; or, The study team has not yet provided the link to the data to the HEAL Data Platform. (To submit a data link, see our documentation about this ) Available: \u201cAvailable\u201d means you are now able to access these data via the HEAL Data Platform, by downloading them and/or by importing them in a workspace Request Access: \u201cRequest Access\u201d means that the data has been deposited in a repository and is available. To access the data, you will first need to visit the host repository to request access to it, following the repository\u2019s standard procedures. Once that is complete, authorized users can then bring these data into a HEAL workspace for analysis . Not Available: \u201cNot Available\u201d means that no data will be made available for this study.","title":"Data Availability Options"},{"location":"platform_discovery_page/#select-files-on-the-discovery-page-and-bring-them-to-the-workspace","text":"The above gif shows the workflow used to select data files from the Discovery Page and bring them into the workspace using Jupyter Notebooks. Log in at https://healdata.org/portal/login . Link your account to all FAIR repositories as described here . Find and select the study by the project number (in this example: 1U2CDA050098-01_a) on the Discovery Page . Find other current open-access studies here . Click the \u201cOpen in Workspace\u201d button in the upper right corner. This step will create a manifest folder which you can find later in the Workspace\u2019s folder \"data/healdata.org\". Select the study and click on \u201cOpen in Workspace\u201d. Choose a workspace flavor and click \"Launch\". This step may take several minutes. Navigate to the /pd/data/healdata.org/ folder and find the placeholder files there. Click on those placeholder files to find instructions how to download the files or see below. From the directory /pd , users can now start a new notebook. Click \u201cNew\u201d (upper right corner) or open a previously saved notebook on the landing drive to load the files into one of the cells, for example by running: ! gen3 drs-pull object \"guid\" import pandas as pd os.chdir('/pd') demo_df = pd.read_csv('pd/demo_file.txt', sep='\\t') demo_df.head() More information for instructions on importing data can be found in the sections \"Download Files to Workspaces\" and \"Workspaces\" . Make sure to terminate the workspace when the work is finished to reduce computational costs. \"pd\" means persistent directory. Saved files outside this directory will be lost. The manifest.json lists metadata of all exported files and can be used to download in batches Note, that all exported data files will be saved in the /pd/data/healdata.org/ folder. Please also note, that the workspace mounts up to a maximum of 5 different manifests while the workspace is running but shows only the latest exported manifest in a newly launched workspace. Terminating the workspace will result in the loss of all but the latests manifest.","title":"Select Files on the Discovery Page and bring them to the Workspace"},{"location":"platform_example_analyses/","text":"Example Analyses \u00b6 The Example Analysis page contains a collection of view-only tutorial Jupyter Notebooks that provide demo analyses of datasets published on the HEAL platform. This tab acts as a 'visual table of contents\u2019 of available HEAL datasets. HEAL Example Analysis \u00b6 This video introduces users to the Example Analysis page, where users can browse Jupyter notebook demos to explore previous data analyses on the HEAL platform. If your Browser does not support watching this video, here's a link to the video instead. Currently available static Jupter Notebooks: Why tutorial Notebooks? \u00b6 These notebooks will allow users to learn how to analyze and visualize data available on the HEAL platform - without having to take the additional steps of finding and exporting the data used by the tutorial first. All tutorial notebooks are static, view-only, and can be downloaded as .html files. Find the notebooks in interactive mode in the Workspace tab . Find below a description of how to work with the tutorials in the workspaces . These tutorial notebooks are meant to: Give users a sense for how the platform can be used to analyze data. Bring complementary datasets together. Be used as a launching point for the users\u2019 own custom analysis. Spark imagination about how users can incorporate the platform data access, analysis, and collaboration tools into their own research and data analysis process and pipelines. The \u201cExample Analysis\u201d tab will be regularly updated with new and exciting tutorial data analysis notebooks, as more datasets are published and brought together in novel ways on the Platform. Currently available Notebooks \u00b6 Find below a list of notebooks that are currently available and which datasets they\u2019re based on. Click here to watch demo videos about how to use the tutorial Jupyter Notebooks as a launching point for your own custom analysis. Please link your account to all FAIR repositories before working with the Notebooks in interactive mode. Notebook Name Project Title Project Number Datasets used Language BACPAC Synthetic Data Analysis Back Pain Consortium (BACPAC) Research Program Data Integration, Algorithm Development and Operations Management Center 1U24AR076730-01 1) participant_ SMART.tsv 2) substance_use_ SMART.tsv 3) physical_function_ SMART.tsv Python JCOIN Tracking Opioid Stigma Methodology and Advanced Analytics Resource Center 1U2CDA050098-01_a 1) JCOIN_NORC_ Omnibus_SURVEY1_ Feb2020.sav 2) JCOIN_NORC_ Omnibus_SURVEY2_ April2020.sav 3) JCOIN_NORC_ Omnibus_SURVEY3_ June2020.sav 4) JCOIN_NORC_ Omnibus_SURVEY4_ Oct2020.sav Python Opioid Prevalence And Overdoses 1) Drug Enforcement Administration Controlled Substances Tracking (DEA ARCOS) 2) Prescription Drug Abuse Policy System 2b) 3) CDC Wide-ranging Online Data for Epidemiologic Research (CDC WONDER) Mortality Multiple Cause-of-Death Public Use Record 1) deaarcos1 2) 57b45d83d6 c9e7e8693ccdfd 3) cdcwonder 1a) dea_arcos_drug_list.tsv 1b) dea_arcos_county_ population.tsv 1c) dea_arcos_combined_ county_annual.tsv 1d) dea_arcos_state_population.tsv 2a) 20170216-RM-Stat-Data.xlsx 2b) Naloxone_Data_09112020.xlsx 2c) 20180810_Good_Samaritan_ Law_Stat_Data.xlsx 3a) CDC_WONDER_unintentional_ overdoses.tsv 3b) CDC_WONDER_suicide_ overdoses.tsv 3c) monthly_unintentional_ overdoses.tsv Python Opioid Environment Toolkit and OEPS R (Watch tutorial) Methodology and Advanced Analytics Resource Center 1U2CDA050098-01_b 1) ZIP_COUNTY.xlsx 2) us-wide-moudsCleaned.csv 3) zctas2018.shp RStudio Opioid Overdose Trajectories (Watch tutorial) CDC Wide-ranging Online Data for Epidemiologic Research (CDC WONDER) Mortality Multiple Cause-of-Death Public Use Record cdcwonder 1) deaths_gender.xlsx 2) deaths_age_cat.xlsx 3) deaths_type_opioid.xlsx 4) cdc_wonder_year_cause_ hedegaard_et_al_2020.txt 5) cdc_wonder_year_cause_ state_hedegaard_et_al_2020.txt Python Working with the Tutorial Notebooks in interactive mode \u00b6 Notebooks require linked access to all FAIR enabled repositories, as described here . Code in the notebooks is editable, and users can import additional datasets and extend their analysis. Demo - How to find data and work on the Tutorial Notebooks \u00b6 Below we describe the steps to export data from the Discovery Page tutorial Jupyter notebooks for one example tutorial notebook BACPAC_Synthetic_Data_Analysis.ipynb (\u201cBACPAC synthetic data analysis\u201d) Users must be logged in and have their accounts linked to the FAIR repositories in order to follow the steps below. Go to the Workspace tab and click \u201cLaunch\u201d on the \u201cBACPAC Synthetic Data Analysis Notebook\u201d. This may take a few minutes to load. Note, that depending on the chosen notebook, this step varies to align with the name of the notebook/workspace. Find and open the notebook in the landing directory by clicking on \u201cBACPAC_Synthetic_Data_Analysis.ipynb\". Click the first two cells and run them by clicking into the cell and either using shift+enter on the keyboard or selecting \u201cRun\u201d. Uncomment the commands as displayed below to install necessary packages. Run the next cell to import the data files. Explore the rest of the notebook. If you want to make changes, make sure to save the notebook as a new file in the directory \"pd\". Make sure to terminate the workspace when the work is finished to reduce computational costs. Note: Work must be saved in the directory \"/pd\" in order to remain accessible after workspace termination. Note, that Workspaces automatically shut down after 90 minutes of idle time.","title":"Example Analysis Page"},{"location":"platform_example_analyses/#example-analyses","text":"The Example Analysis page contains a collection of view-only tutorial Jupyter Notebooks that provide demo analyses of datasets published on the HEAL platform. This tab acts as a 'visual table of contents\u2019 of available HEAL datasets.","title":"Example Analyses"},{"location":"platform_example_analyses/#heal-example-analysis","text":"This video introduces users to the Example Analysis page, where users can browse Jupyter notebook demos to explore previous data analyses on the HEAL platform. If your Browser does not support watching this video, here's a link to the video instead. Currently available static Jupter Notebooks:","title":"HEAL Example Analysis"},{"location":"platform_example_analyses/#why-tutorial-notebooks","text":"These notebooks will allow users to learn how to analyze and visualize data available on the HEAL platform - without having to take the additional steps of finding and exporting the data used by the tutorial first. All tutorial notebooks are static, view-only, and can be downloaded as .html files. Find the notebooks in interactive mode in the Workspace tab . Find below a description of how to work with the tutorials in the workspaces . These tutorial notebooks are meant to: Give users a sense for how the platform can be used to analyze data. Bring complementary datasets together. Be used as a launching point for the users\u2019 own custom analysis. Spark imagination about how users can incorporate the platform data access, analysis, and collaboration tools into their own research and data analysis process and pipelines. The \u201cExample Analysis\u201d tab will be regularly updated with new and exciting tutorial data analysis notebooks, as more datasets are published and brought together in novel ways on the Platform.","title":"Why tutorial Notebooks?"},{"location":"platform_example_analyses/#currently-available-notebooks","text":"Find below a list of notebooks that are currently available and which datasets they\u2019re based on. Click here to watch demo videos about how to use the tutorial Jupyter Notebooks as a launching point for your own custom analysis. Please link your account to all FAIR repositories before working with the Notebooks in interactive mode. Notebook Name Project Title Project Number Datasets used Language BACPAC Synthetic Data Analysis Back Pain Consortium (BACPAC) Research Program Data Integration, Algorithm Development and Operations Management Center 1U24AR076730-01 1) participant_ SMART.tsv 2) substance_use_ SMART.tsv 3) physical_function_ SMART.tsv Python JCOIN Tracking Opioid Stigma Methodology and Advanced Analytics Resource Center 1U2CDA050098-01_a 1) JCOIN_NORC_ Omnibus_SURVEY1_ Feb2020.sav 2) JCOIN_NORC_ Omnibus_SURVEY2_ April2020.sav 3) JCOIN_NORC_ Omnibus_SURVEY3_ June2020.sav 4) JCOIN_NORC_ Omnibus_SURVEY4_ Oct2020.sav Python Opioid Prevalence And Overdoses 1) Drug Enforcement Administration Controlled Substances Tracking (DEA ARCOS) 2) Prescription Drug Abuse Policy System 2b) 3) CDC Wide-ranging Online Data for Epidemiologic Research (CDC WONDER) Mortality Multiple Cause-of-Death Public Use Record 1) deaarcos1 2) 57b45d83d6 c9e7e8693ccdfd 3) cdcwonder 1a) dea_arcos_drug_list.tsv 1b) dea_arcos_county_ population.tsv 1c) dea_arcos_combined_ county_annual.tsv 1d) dea_arcos_state_population.tsv 2a) 20170216-RM-Stat-Data.xlsx 2b) Naloxone_Data_09112020.xlsx 2c) 20180810_Good_Samaritan_ Law_Stat_Data.xlsx 3a) CDC_WONDER_unintentional_ overdoses.tsv 3b) CDC_WONDER_suicide_ overdoses.tsv 3c) monthly_unintentional_ overdoses.tsv Python Opioid Environment Toolkit and OEPS R (Watch tutorial) Methodology and Advanced Analytics Resource Center 1U2CDA050098-01_b 1) ZIP_COUNTY.xlsx 2) us-wide-moudsCleaned.csv 3) zctas2018.shp RStudio Opioid Overdose Trajectories (Watch tutorial) CDC Wide-ranging Online Data for Epidemiologic Research (CDC WONDER) Mortality Multiple Cause-of-Death Public Use Record cdcwonder 1) deaths_gender.xlsx 2) deaths_age_cat.xlsx 3) deaths_type_opioid.xlsx 4) cdc_wonder_year_cause_ hedegaard_et_al_2020.txt 5) cdc_wonder_year_cause_ state_hedegaard_et_al_2020.txt Python","title":"Currently available Notebooks"},{"location":"platform_example_analyses/#working-with-the-tutorial-notebooks-in-interactive-mode","text":"Notebooks require linked access to all FAIR enabled repositories, as described here . Code in the notebooks is editable, and users can import additional datasets and extend their analysis.","title":"Working with the Tutorial Notebooks in interactive mode"},{"location":"platform_example_analyses/#demo-how-to-find-data-and-work-on-the-tutorial-notebooks","text":"Below we describe the steps to export data from the Discovery Page tutorial Jupyter notebooks for one example tutorial notebook BACPAC_Synthetic_Data_Analysis.ipynb (\u201cBACPAC synthetic data analysis\u201d) Users must be logged in and have their accounts linked to the FAIR repositories in order to follow the steps below. Go to the Workspace tab and click \u201cLaunch\u201d on the \u201cBACPAC Synthetic Data Analysis Notebook\u201d. This may take a few minutes to load. Note, that depending on the chosen notebook, this step varies to align with the name of the notebook/workspace. Find and open the notebook in the landing directory by clicking on \u201cBACPAC_Synthetic_Data_Analysis.ipynb\". Click the first two cells and run them by clicking into the cell and either using shift+enter on the keyboard or selecting \u201cRun\u201d. Uncomment the commands as displayed below to install necessary packages. Run the next cell to import the data files. Explore the rest of the notebook. If you want to make changes, make sure to save the notebook as a new file in the directory \"pd\". Make sure to terminate the workspace when the work is finished to reduce computational costs. Note: Work must be saved in the directory \"/pd\" in order to remain accessible after workspace termination. Note, that Workspaces automatically shut down after 90 minutes of idle time.","title":"Demo - How to find data and work on the Tutorial Notebooks"},{"location":"platform_profile_page/","text":"Profile Page \u00b6 On the profile page users will find information regarding their access to projects, access to Gen3-specific tools (e.g. access to the /workspace), and the function to create API keys for credential downloads. API keys are necessary for the downloadof files using the Gen3-client; for more information see chapter 4. Users can view their study access in the Profile Page. API keys can be viewed, created, and downloaded on the Profile Page.","title":"Profile Page"},{"location":"platform_profile_page/#profile-page","text":"On the profile page users will find information regarding their access to projects, access to Gen3-specific tools (e.g. access to the /workspace), and the function to create API keys for credential downloads. API keys are necessary for the downloadof files using the Gen3-client; for more information see chapter 4. Users can view their study access in the Profile Page. API keys can be viewed, created, and downloaded on the Profile Page.","title":"Profile Page"},{"location":"platform_request_access/","text":"Check and request access \u00b6 Users can find out for which studies they have access to data from the platform by navigating to the Discovery Page and selecting \u201cData Availability\" (#1), then \"Available\" (#2), then \u201dOK\u201d (#3), as shown below. Note: users should log in before they try to filter for studies with access to make sure that they are maximizing studies for which they have data access. If you also check the Request Access box, you will identify additional studies with data for which you can request access. (see our documentation about the Data Availability filter ) Access to individual Studies \u00b6 You can also check access by clicking on a study in the Discovery Page (#1), then clicking on the Data tab in the study page (#2), as shown below. The Data tab will display access permissions in the Data Files section. If you have access, a green box will show \u201cYou have access to this study\u201d. Current (open-access) studies \u00b6 Current open-access studies will be shown when navigating to the Discovery Page. Users can download and/or export open-access study files after logging in. Currently, the HEAL Platform hosts the following studies. All are open-access except when noted. Note, that different studies relate to different Data Resources . Project number Study Name Data Resource 57b45d83d6c9e7e8693ccdfd Naloxone Overdose Prevention Laws PDAPS 10.3886/ICPSR34945.v3 National Mental Health Services Survey (N-MHSS) ICPSR 1U24AR076730-01 A synthetic dataset from the Back Pain Consortium (BACPAC) Research Program Data Integration, Algorithm Development and Operations Management Center HEAL 10.3886/ICPSR04256.v5 National Survey of Substance Abuse Treatment Services (N-SSATS) ICPSR cdcwonder CDC Wide-ranging Online Data for Epidemiologic Research (CDC WONDER) Mortality Multiple Cause-of-Death Public Use Record HEAL 10.3886/ICPSR30122.v5 Treatment Episode Data Set: Discharges (TEDS-D) ICPSR deaarcos1 Drug Enforcement Administration Controlled Substances Tracking (DEA ARCOS) HEAL a) 1U2CDA050098-01_a JCOIN - Methodology and Advanced Analytics Resource Center: JCOIN b) 1U2CDA050098-01_b a) JCOIN 026: Amerispeak Brief Stigma Survey JCOIN b) The Opioid Environment Policy Scan JCOIN Overview of access to Datasets and Studies \u00b6 Users can visit the \"Profile\u201d page to view a list of studies they have access to under the \u201cYou have access to the following project(s)\u201d section, as shown in the figure below. A user can view their study access in the Profile Page. From here users can also view current API keys or create and download new keys. API keys are required for downloads which require use of the Gen3-client. More information in the chapter \u201cDownloading Data Files\u201d . API keys can be viewed, created, and downloaded on the Profile Page. From here users can also view if they have access to projects or workspaces /dictionary_page: You have access to the data dictionary. /workspace: You have access to the workspace. /programs/open: You have access to open-access projects. Linking access to FAIR enabled repositories \u00b6 The HEAL Platform securely exposes data stored on multiple HEAL-compliant FAIR repositories . Users need to link their account to currently all FAIR repositories in order to: run Jupyter Notebooks that utilize data stored on various FAIR repositories. export data that are stored on FAIR repositories from the Discovery Page to the Workspaces . download data that are stored on FAIR repositories from the Discovery Page . In order to link the account to the involved FAIR repositories, navigate to the Profile Page and link the account to all current login options by clicking on the \"Refresh [..] Google Login\" buttons as shown below. Linking access options on the Profile Page for data stored on FAIR enabled repositories. Access needs to be renewed after 30 days, as indicated after \"Status: expires in [..] days\". As a reminder, users will be prompted with a banner on the Workspace page and a pop-up window on the Discovery page . Users are reminded on the Workspace page to link the account to all other FAIR enabled repositories on the Profile Page. Users are reminded on the Discovery page to link the account to all other FAIR enabled repositories.","title":"Access check/request"},{"location":"platform_request_access/#check-and-request-access","text":"Users can find out for which studies they have access to data from the platform by navigating to the Discovery Page and selecting \u201cData Availability\" (#1), then \"Available\" (#2), then \u201dOK\u201d (#3), as shown below. Note: users should log in before they try to filter for studies with access to make sure that they are maximizing studies for which they have data access. If you also check the Request Access box, you will identify additional studies with data for which you can request access. (see our documentation about the Data Availability filter )","title":"Check and request access"},{"location":"platform_request_access/#access-to-individual-studies","text":"You can also check access by clicking on a study in the Discovery Page (#1), then clicking on the Data tab in the study page (#2), as shown below. The Data tab will display access permissions in the Data Files section. If you have access, a green box will show \u201cYou have access to this study\u201d.","title":"Access to individual Studies"},{"location":"platform_request_access/#current-open-access-studies","text":"Current open-access studies will be shown when navigating to the Discovery Page. Users can download and/or export open-access study files after logging in. Currently, the HEAL Platform hosts the following studies. All are open-access except when noted. Note, that different studies relate to different Data Resources . Project number Study Name Data Resource 57b45d83d6c9e7e8693ccdfd Naloxone Overdose Prevention Laws PDAPS 10.3886/ICPSR34945.v3 National Mental Health Services Survey (N-MHSS) ICPSR 1U24AR076730-01 A synthetic dataset from the Back Pain Consortium (BACPAC) Research Program Data Integration, Algorithm Development and Operations Management Center HEAL 10.3886/ICPSR04256.v5 National Survey of Substance Abuse Treatment Services (N-SSATS) ICPSR cdcwonder CDC Wide-ranging Online Data for Epidemiologic Research (CDC WONDER) Mortality Multiple Cause-of-Death Public Use Record HEAL 10.3886/ICPSR30122.v5 Treatment Episode Data Set: Discharges (TEDS-D) ICPSR deaarcos1 Drug Enforcement Administration Controlled Substances Tracking (DEA ARCOS) HEAL a) 1U2CDA050098-01_a JCOIN - Methodology and Advanced Analytics Resource Center: JCOIN b) 1U2CDA050098-01_b a) JCOIN 026: Amerispeak Brief Stigma Survey JCOIN b) The Opioid Environment Policy Scan JCOIN","title":"Current (open-access) studies"},{"location":"platform_request_access/#overview-of-access-to-datasets-and-studies","text":"Users can visit the \"Profile\u201d page to view a list of studies they have access to under the \u201cYou have access to the following project(s)\u201d section, as shown in the figure below. A user can view their study access in the Profile Page. From here users can also view current API keys or create and download new keys. API keys are required for downloads which require use of the Gen3-client. More information in the chapter \u201cDownloading Data Files\u201d . API keys can be viewed, created, and downloaded on the Profile Page. From here users can also view if they have access to projects or workspaces /dictionary_page: You have access to the data dictionary. /workspace: You have access to the workspace. /programs/open: You have access to open-access projects.","title":"Overview of access to Datasets and Studies"},{"location":"platform_request_access/#linking-access-to-fair-enabled-repositories","text":"The HEAL Platform securely exposes data stored on multiple HEAL-compliant FAIR repositories . Users need to link their account to currently all FAIR repositories in order to: run Jupyter Notebooks that utilize data stored on various FAIR repositories. export data that are stored on FAIR repositories from the Discovery Page to the Workspaces . download data that are stored on FAIR repositories from the Discovery Page . In order to link the account to the involved FAIR repositories, navigate to the Profile Page and link the account to all current login options by clicking on the \"Refresh [..] Google Login\" buttons as shown below. Linking access options on the Profile Page for data stored on FAIR enabled repositories. Access needs to be renewed after 30 days, as indicated after \"Status: expires in [..] days\". As a reminder, users will be prompted with a banner on the Workspace page and a pop-up window on the Discovery page . Users are reminded on the Workspace page to link the account to all other FAIR enabled repositories on the Profile Page. Users are reminded on the Discovery page to link the account to all other FAIR enabled repositories.","title":"Linking access to FAIR enabled repositories"},{"location":"platform_tutorial_videos/","text":"Tutorial Videos \u00b6 Watch our tutorial videos to learn how to interact with the HEAL Platform, export data files from the Discovery Page, and use the tutorial Jupyter Notebooks as a launching point for your own custom analysis. HEAL Platform Tutorial \u00b6 Watch this video to start learning about all features on the HEAL Platform. If your Browser does not support watching this video, here's a link to the video instead. The Opioid Environment Toolkit and OEPS \u00b6 This tutorial video demonstrates the use of the HEAL Platform Workspace through the analysis for the Opioid Environment Policy Scan Dataset using statistical tools written in the R programming language. If your Browser does not support watching this video, here's a link to the video instead. Opioid Overdose Trajectories \u00b6 This tutorial video demonstrates the use of the HEAL Platform to reproduce findings of a CDC study of Opioid Overdose Death rates in the United States. If your Browser does not support watching this video, here's a link to the video instead. HEAL Example Analysis \u00b6 This video introduces users to the Example Analysis page, where users can browse Jupyter notebook demos to explore previous data analyses on the HEAL platform. If your Browser does not support watching this video, here's a link to the video instead.","title":"Tutorial Videos"},{"location":"platform_tutorial_videos/#tutorial-videos","text":"Watch our tutorial videos to learn how to interact with the HEAL Platform, export data files from the Discovery Page, and use the tutorial Jupyter Notebooks as a launching point for your own custom analysis.","title":"Tutorial Videos"},{"location":"platform_tutorial_videos/#heal-platform-tutorial","text":"Watch this video to start learning about all features on the HEAL Platform. If your Browser does not support watching this video, here's a link to the video instead.","title":"HEAL Platform Tutorial"},{"location":"platform_tutorial_videos/#the-opioid-environment-toolkit-and-oeps","text":"This tutorial video demonstrates the use of the HEAL Platform Workspace through the analysis for the Opioid Environment Policy Scan Dataset using statistical tools written in the R programming language. If your Browser does not support watching this video, here's a link to the video instead.","title":"The Opioid Environment Toolkit and OEPS"},{"location":"platform_tutorial_videos/#opioid-overdose-trajectories","text":"This tutorial video demonstrates the use of the HEAL Platform to reproduce findings of a CDC study of Opioid Overdose Death rates in the United States. If your Browser does not support watching this video, here's a link to the video instead.","title":"Opioid Overdose Trajectories"},{"location":"platform_tutorial_videos/#heal-example-analysis","text":"This video introduces users to the Example Analysis page, where users can browse Jupyter notebook demos to explore previous data analyses on the HEAL platform. If your Browser does not support watching this video, here's a link to the video instead.","title":"HEAL Example Analysis"},{"location":"platform_workspaces/","text":"Workspaces \u00b6 HEAL Platform workspaces are secure data analysis environments in the cloud that can access data from one or more data resources. Workspaces include Jupyter notebooks, Python, and RStudio by default but can be configured to host virtually any application, including analysis workflows, data processing pipelines, or data visualization apps. In order to launch a workspace, users will need to request a Gen3 workspace account at \"https://healportal.org/\". There are two methods to support workspaces: grant-funded accounts paid for by your institution (STRIDES grant) or a STRIDES credit account supported by the NIH initiative. Note, it may take several days for your account to be approved . New to Jupyter? Learn more about the popular tool for data scientists on Jupyter.org (disclaimer: CTDS is not responsible for the content). Guideline to get started \u00b6 Info To use workspaces, you must first register for workspace access as described on the Workspace Registration page . After navigating to https://healdata.org/portal/workspace , users will discover a list of pre-configured virtual machine (VM) images, as shown below. Available workspaces on the HEAL Platform (top). Users may need to link their accounts from other repositories (bottom); click here to see how . (Generic) Jupyter Notebook with R kernel: Choose this VM if you are familiar with setting up Python- or R-based Notebooks, or if you just exported one or multiple studies from the Discovery Page and want to start your custom analysis. (Generic, User-licensed) Stata Notebook: Choose this VM if you are familiar with Stata-based data analysis. This notebook requires a Stata license. Tutorial Notebooks: Explore our Jupyter Notebook tutorials written in Python or RStudio, which pull data from various sources of the HEAL Data Ecosystem to leverage statistical programs and data analysis tools. All interactive tutorial notebooks can be also found as static version on the Notebook Browser tab ; read more in the section \"Example Analysis\" . Feel free to edit and experiment with this collection of notebooks. They are your personal copies! Notebooks in Python : (1) BACPAC Synthetic Data Analysis, (2) JCOIN Tracking Opioid Stigma, (3) Opioid Overdose Trajectories, (4) Opioid Prevalence And Overdoses Notebooks in RStudio : (1) Opioid Environment Toolkit and OEPS Click \u201cLaunch\u201d on any of the above workspace flavors to spin up a copy of that VM. Note: Launching the VM may take several minutes. The status of launching the workspace is displayed after clicking on \u201cLaunch\u201d. After launching, the home folders are displayed, one of which is the user's persistent drive (\"pd\"). The /pd directory is a user\u2019s persistent drive. Select the /pd folder. Only files saved in the /pd directory will remain available after termination of a workspace session. New files or licenses should be saved in the the /pd directory if users need to access them after restarting the workspaces. Attention: Any personal files in the folder \u201cdata\u201d will be lost. Personal files in the directory /pd will persist. Do not save files in the \"data\" and \u201cdata/healdata.org\u201d folders. The folder \u201chealdata.org\u201d in the \u201cdata\u201d folder will host the data files you have exported from the Discovery Page. Start a new notebook by clicking \u201cNew\u201d in the top right corner and choose between Python 3 or R Studio as the base programmatic language. Start a new notebook under \u201cNew\u201d. Experiment away! Code blocks are entered in cells, which can be executed individually or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. Users can import data files directly into the Notebook code after selecting files from the \"Discovery Page\" . An example is shown below. Do not forget to terminate your workspace once your work is finished to be mindful of the cost-intensive computational effort. Note, that Workspaces automatically shut down after 90 minutes of idle time. Do not forget to terminate your workspace once your work is finished. Unterminated workspaces continue to accrue computational costs. Further reading: read more about how to download data files into the Workspaces here . Upload, save, and download Files/Notebooks \u00b6 Users can upload data files or Notebooks from the local machine to the home directory by clicking on \u201cUpload\u201d and access them in the Notebook (see below). Upload data files or Notebooks to the workspace by clicking on \u201cUpload\u201d in the top right corner. On the upload screen, find your text file (in this example, it is \"this\\_is\\_a\\_demo.txt\") to upload from your local computer. Then run in the cells, for example: import os import pandas as pd os.chdir('/data') demo_df = pd.read_csv('/this_is_a_demo.txt', sep='\\t') demo_df.head() Users can save the notebook by clicking \"File\" - \"Save as\", as shown below. Save the notebook under \u201cFile\u201d - \"Save as\". Users can download notebooks by clicking \"File\" - \"Download as\", as shown below. Download the notebook as \".ipynb\". Environments, Languages, and Tools \u00b6 The following environments are available in the workspaces: Jupyter RStudio The following programmatic languages are available in Jupyter Notebooks: RStudio ( read more ) Python 3 ( read more ) Stata ( read more ) The following tools are available in Jupyter Notebooks: GitHub ( read GitHub documentation ) Python 3 and RStudio in Jupyter \u00b6 Both Python 3 and RStudio are available in Jupyter Notebooks. Users can expect to be able to use typical Python or RStudio packages, such as PyPI or CRAN. For Python and RStudio, users can start a new notebook under \"New\", as shown below. Find Python 3 or RStudio when starting a new notebook under \u201cNew\u201d. Stata in Jupyter \u00b6 Stata is available as language in Jupyter notebooks (either in Python or R kernels), but requires a license and a specific workspace. Users need to first choose the following workspace \"(Generic, User-licensed) Stata Notebook\" in order to be able to use Stata: Select this workspace to use Stata in the notebook. Users need to upload a license stata.lic to the /pd folder by selecting \"Upload\" (top right). If the upload was successful, the license appears in the directory /pd. Note , that uploading the license can also be achieved programmatically by opening a new terminal window under \"New\" - \"Terminal\", finding the directory /pd by typing: cd pd . Then, create a file using vim: vim stata.lic . This will open the file in the terminal. Users can copy the license, then hit : + wq . Then, users need to start a new notebook under \"New\" (choose either R or Python). Run the following code in the first cell: import stata_setup stata_setup.config(\"/usr/local/stata17\", \"mp\") This will return the following: Setup the license in the first cell. Users can then begin using the notebook by typing in known Stata commands, for example %% stata . describe . Troubleshooting \u00b6 If the kernel died, make sure to be logged in on 1) the Login Page 2) have enabled access to the FAIR enabled repository . Automatic Workspace Shutdown \u00b6 Workspaces automatically shut down after 90 minutes of idle time and a pop-up window will remind users before the workspace shuts down. A pop-up window will remind users to navigate back to the workspaces page in order to save the data. After the workspace has been shut down, users will be notified with the following pop-up window. Workspaces automatically shut down after 90 minutes of idle time.","title":"Workspaces"},{"location":"platform_workspaces/#workspaces","text":"HEAL Platform workspaces are secure data analysis environments in the cloud that can access data from one or more data resources. Workspaces include Jupyter notebooks, Python, and RStudio by default but can be configured to host virtually any application, including analysis workflows, data processing pipelines, or data visualization apps. In order to launch a workspace, users will need to request a Gen3 workspace account at \"https://healportal.org/\". There are two methods to support workspaces: grant-funded accounts paid for by your institution (STRIDES grant) or a STRIDES credit account supported by the NIH initiative. Note, it may take several days for your account to be approved . New to Jupyter? Learn more about the popular tool for data scientists on Jupyter.org (disclaimer: CTDS is not responsible for the content).","title":"Workspaces"},{"location":"platform_workspaces/#guideline-to-get-started","text":"Info To use workspaces, you must first register for workspace access as described on the Workspace Registration page . After navigating to https://healdata.org/portal/workspace , users will discover a list of pre-configured virtual machine (VM) images, as shown below. Available workspaces on the HEAL Platform (top). Users may need to link their accounts from other repositories (bottom); click here to see how . (Generic) Jupyter Notebook with R kernel: Choose this VM if you are familiar with setting up Python- or R-based Notebooks, or if you just exported one or multiple studies from the Discovery Page and want to start your custom analysis. (Generic, User-licensed) Stata Notebook: Choose this VM if you are familiar with Stata-based data analysis. This notebook requires a Stata license. Tutorial Notebooks: Explore our Jupyter Notebook tutorials written in Python or RStudio, which pull data from various sources of the HEAL Data Ecosystem to leverage statistical programs and data analysis tools. All interactive tutorial notebooks can be also found as static version on the Notebook Browser tab ; read more in the section \"Example Analysis\" . Feel free to edit and experiment with this collection of notebooks. They are your personal copies! Notebooks in Python : (1) BACPAC Synthetic Data Analysis, (2) JCOIN Tracking Opioid Stigma, (3) Opioid Overdose Trajectories, (4) Opioid Prevalence And Overdoses Notebooks in RStudio : (1) Opioid Environment Toolkit and OEPS Click \u201cLaunch\u201d on any of the above workspace flavors to spin up a copy of that VM. Note: Launching the VM may take several minutes. The status of launching the workspace is displayed after clicking on \u201cLaunch\u201d. After launching, the home folders are displayed, one of which is the user's persistent drive (\"pd\"). The /pd directory is a user\u2019s persistent drive. Select the /pd folder. Only files saved in the /pd directory will remain available after termination of a workspace session. New files or licenses should be saved in the the /pd directory if users need to access them after restarting the workspaces. Attention: Any personal files in the folder \u201cdata\u201d will be lost. Personal files in the directory /pd will persist. Do not save files in the \"data\" and \u201cdata/healdata.org\u201d folders. The folder \u201chealdata.org\u201d in the \u201cdata\u201d folder will host the data files you have exported from the Discovery Page. Start a new notebook by clicking \u201cNew\u201d in the top right corner and choose between Python 3 or R Studio as the base programmatic language. Start a new notebook under \u201cNew\u201d. Experiment away! Code blocks are entered in cells, which can be executed individually or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. Users can import data files directly into the Notebook code after selecting files from the \"Discovery Page\" . An example is shown below. Do not forget to terminate your workspace once your work is finished to be mindful of the cost-intensive computational effort. Note, that Workspaces automatically shut down after 90 minutes of idle time. Do not forget to terminate your workspace once your work is finished. Unterminated workspaces continue to accrue computational costs. Further reading: read more about how to download data files into the Workspaces here .","title":"Guideline to get started"},{"location":"platform_workspaces/#upload-save-and-download-filesnotebooks","text":"Users can upload data files or Notebooks from the local machine to the home directory by clicking on \u201cUpload\u201d and access them in the Notebook (see below). Upload data files or Notebooks to the workspace by clicking on \u201cUpload\u201d in the top right corner. On the upload screen, find your text file (in this example, it is \"this\\_is\\_a\\_demo.txt\") to upload from your local computer. Then run in the cells, for example: import os import pandas as pd os.chdir('/data') demo_df = pd.read_csv('/this_is_a_demo.txt', sep='\\t') demo_df.head() Users can save the notebook by clicking \"File\" - \"Save as\", as shown below. Save the notebook under \u201cFile\u201d - \"Save as\". Users can download notebooks by clicking \"File\" - \"Download as\", as shown below. Download the notebook as \".ipynb\".","title":"Upload, save, and download Files/Notebooks"},{"location":"platform_workspaces/#environments-languages-and-tools","text":"The following environments are available in the workspaces: Jupyter RStudio The following programmatic languages are available in Jupyter Notebooks: RStudio ( read more ) Python 3 ( read more ) Stata ( read more ) The following tools are available in Jupyter Notebooks: GitHub ( read GitHub documentation )","title":"Environments, Languages, and Tools"},{"location":"platform_workspaces/#python-3-and-rstudio-in-jupyter","text":"Both Python 3 and RStudio are available in Jupyter Notebooks. Users can expect to be able to use typical Python or RStudio packages, such as PyPI or CRAN. For Python and RStudio, users can start a new notebook under \"New\", as shown below. Find Python 3 or RStudio when starting a new notebook under \u201cNew\u201d.","title":"Python 3 and RStudio in Jupyter"},{"location":"platform_workspaces/#stata-in-jupyter","text":"Stata is available as language in Jupyter notebooks (either in Python or R kernels), but requires a license and a specific workspace. Users need to first choose the following workspace \"(Generic, User-licensed) Stata Notebook\" in order to be able to use Stata: Select this workspace to use Stata in the notebook. Users need to upload a license stata.lic to the /pd folder by selecting \"Upload\" (top right). If the upload was successful, the license appears in the directory /pd. Note , that uploading the license can also be achieved programmatically by opening a new terminal window under \"New\" - \"Terminal\", finding the directory /pd by typing: cd pd . Then, create a file using vim: vim stata.lic . This will open the file in the terminal. Users can copy the license, then hit : + wq . Then, users need to start a new notebook under \"New\" (choose either R or Python). Run the following code in the first cell: import stata_setup stata_setup.config(\"/usr/local/stata17\", \"mp\") This will return the following: Setup the license in the first cell. Users can then begin using the notebook by typing in known Stata commands, for example %% stata . describe .","title":"Stata in Jupyter"},{"location":"platform_workspaces/#troubleshooting","text":"If the kernel died, make sure to be logged in on 1) the Login Page 2) have enabled access to the FAIR enabled repository .","title":"Troubleshooting"},{"location":"platform_workspaces/#automatic-workspace-shutdown","text":"Workspaces automatically shut down after 90 minutes of idle time and a pop-up window will remind users before the workspace shuts down. A pop-up window will remind users to navigate back to the workspaces page in order to save the data. After the workspace has been shut down, users will be notified with the following pop-up window. Workspaces automatically shut down after 90 minutes of idle time.","title":"Automatic Workspace Shutdown"},{"location":"reporting-repo/","text":"Reporting Repository Selection and Data Availability \u00b6 Selecting a HEAL-compliant repository \u00b6 There are several considerations when selecting the best repository for your data. We encourage you to review the Repository Selection Guide to view the HEAL-compliant repositories and their features and characteristics. HEAL investigators can get assistance with selecting a repository by contacting the HEAL Data Stewards . How can I tell if my study shows a repository selection on the platform? \u00b6 You can check the HEAL Data Platform to see if we have a data repository selection recorded for your study. Click on the Discovery button, or go to https://healdata.org/portal/discovery Search for your study (e.g., by project number, name, or PI) Find your study in the search results. At the bottom of the study row, there may be multiple color-bordered boxes. If we have your repo selection on the platform, it will appear in one of these boxes (generally the first, but does not have to be). The red arrow below shows an example of a study that selected Dryad as their repository. My study doesn't show the repository I selected. How can I update that? \u00b6 If you need to report the repository(ies) that you have selected, there are two options to do so: If you have not yet registered your study: You will have the option to indicate your repository selection when you register your study. You can follow the instructions here to register . If you have any trouble or questions, reach out to us at heal-support@gen3.org . If you have already registered your study, or you have more than one repository selection: You can email us at heal-support@gen3.org to report your repository selection(s). In your email, please include the permalink to your study page on the platform so we know exactly which study needs to be updated. Get the permalink to your study \u00b6 A permalink is a permanent link to your study page -- it makes it quick and easy to open the page without searching. The study's permalink includes the HEAL Data Platform ID (HDP ID), a unique identifier for your study, at the end of the URL. To get the permalink to your study, you can do the following: Click on the Discovery button, or go to https://healdata.org/portal/discovery Search for your study (e.g., by project number, name, or PI) Click on the study in the search results In the window that opens with the study page details, there is a button in the upper right corner that says Permalink. Click the permalink button to copy the direct link to the study details page. Reporting that data is now available \u00b6 If you have deposited your data in a HEAL-compliant repository, we want to link to your data from the HEAL Data Platform. Please email us at heal-support@gen3.org to share your data link(s). In your email, please include the permalink to the platform study page (see above).","title":"Reporting Repository Selection and Data Availability"},{"location":"reporting-repo/#reporting-repository-selection-and-data-availability","text":"","title":"Reporting Repository Selection and Data Availability"},{"location":"reporting-repo/#selecting-a-heal-compliant-repository","text":"There are several considerations when selecting the best repository for your data. We encourage you to review the Repository Selection Guide to view the HEAL-compliant repositories and their features and characteristics. HEAL investigators can get assistance with selecting a repository by contacting the HEAL Data Stewards .","title":"Selecting a HEAL-compliant repository"},{"location":"reporting-repo/#how-can-i-tell-if-my-study-shows-a-repository-selection-on-the-platform","text":"You can check the HEAL Data Platform to see if we have a data repository selection recorded for your study. Click on the Discovery button, or go to https://healdata.org/portal/discovery Search for your study (e.g., by project number, name, or PI) Find your study in the search results. At the bottom of the study row, there may be multiple color-bordered boxes. If we have your repo selection on the platform, it will appear in one of these boxes (generally the first, but does not have to be). The red arrow below shows an example of a study that selected Dryad as their repository.","title":"How can I tell if my study shows a repository selection on the platform?"},{"location":"reporting-repo/#my-study-doesnt-show-the-repository-i-selected-how-can-i-update-that","text":"If you need to report the repository(ies) that you have selected, there are two options to do so: If you have not yet registered your study: You will have the option to indicate your repository selection when you register your study. You can follow the instructions here to register . If you have any trouble or questions, reach out to us at heal-support@gen3.org . If you have already registered your study, or you have more than one repository selection: You can email us at heal-support@gen3.org to report your repository selection(s). In your email, please include the permalink to your study page on the platform so we know exactly which study needs to be updated.","title":"My study doesn't show the repository I selected. How can I update that?"},{"location":"reporting-repo/#get-the-permalink-to-your-study","text":"A permalink is a permanent link to your study page -- it makes it quick and easy to open the page without searching. The study's permalink includes the HEAL Data Platform ID (HDP ID), a unique identifier for your study, at the end of the URL. To get the permalink to your study, you can do the following: Click on the Discovery button, or go to https://healdata.org/portal/discovery Search for your study (e.g., by project number, name, or PI) Click on the study in the search results In the window that opens with the study page details, there is a button in the upper right corner that says Permalink. Click the permalink button to copy the direct link to the study details page.","title":"Get the permalink to your study"},{"location":"reporting-repo/#reporting-that-data-is-now-available","text":"If you have deposited your data in a HEAL-compliant repository, we want to link to your data from the HEAL Data Platform. Please email us at heal-support@gen3.org to share your data link(s). In your email, please include the permalink to the platform study page (see above).","title":"Reporting that data is now available"},{"location":"slmd_submission/","text":"Study-Level Metadata Submission \u00b6 Info There is detailed guidance available for completing the CEDAR metadata form. CEDAR Form Guidance Fill out your CEDAR form \u00b6 The act of submitting the study registration form will result in the creation of a metadata input form within your CEDAR account: Find the form for your study under the 'Shared with Me' folder on CEDAR (accessible in the left-hand navigation). When you enter additional data into the CEDAR form, be sure to SAVE your changes by scrolling to the bottom of the form. The HEAL Data Platform will pull entries from the CEDAR templates into the Platform to enhance search capabilities and provide increasingly robust study details. Watch our tutorial videos to learn how to interact with the CEDAR metadata forms and submit study-level metadata. Why Metadata are Important, & Finding Your Study Metadata Form \u00b6 This video covers: Why completing the metadata form is important How to find the metadata form How to share the metadata form Note This video is to be watched after you have registered your study with the Platform. If you have not yet registered your study, we have instructional videos to help you with this process. Resources of interest mentioned in this video include: Fresh FAIR Webinar Series Video - Metadata 101 Fresh FAIR Webinar Series Video - Metadata 102 Other resources: healdatafair.org/resources/metadata Contact address for help: heal-support@gen3.org You can also view this video on YouTube . Completing the Study Metadata Form \u00b6 This video provides a step-by-step guide for completing the HEAL study-level metadata form. Resources of interest mentioned in this video include: Other metadata resources: healdatafair.org/resources/metadata Contact address for help: heal-support@gen3.org You can also view this video on YouTube .","title":"Study-Level Metadata Submission"},{"location":"slmd_submission/#study-level-metadata-submission","text":"Info There is detailed guidance available for completing the CEDAR metadata form. CEDAR Form Guidance","title":"Study-Level Metadata Submission"},{"location":"slmd_submission/#fill-out-your-cedar-form","text":"The act of submitting the study registration form will result in the creation of a metadata input form within your CEDAR account: Find the form for your study under the 'Shared with Me' folder on CEDAR (accessible in the left-hand navigation). When you enter additional data into the CEDAR form, be sure to SAVE your changes by scrolling to the bottom of the form. The HEAL Data Platform will pull entries from the CEDAR templates into the Platform to enhance search capabilities and provide increasingly robust study details. Watch our tutorial videos to learn how to interact with the CEDAR metadata forms and submit study-level metadata.","title":"Fill out your CEDAR form"},{"location":"slmd_submission/#why-metadata-are-important-finding-your-study-metadata-form","text":"This video covers: Why completing the metadata form is important How to find the metadata form How to share the metadata form Note This video is to be watched after you have registered your study with the Platform. If you have not yet registered your study, we have instructional videos to help you with this process. Resources of interest mentioned in this video include: Fresh FAIR Webinar Series Video - Metadata 101 Fresh FAIR Webinar Series Video - Metadata 102 Other resources: healdatafair.org/resources/metadata Contact address for help: heal-support@gen3.org You can also view this video on YouTube .","title":"Why Metadata are Important, &amp; Finding Your Study Metadata Form"},{"location":"slmd_submission/#completing-the-study-metadata-form","text":"This video provides a step-by-step guide for completing the HEAL study-level metadata form. Resources of interest mentioned in this video include: Other metadata resources: healdatafair.org/resources/metadata Contact address for help: heal-support@gen3.org You can also view this video on YouTube .","title":"Completing the Study Metadata Form"},{"location":"study-registration/","text":"Study Registration \u00b6 Registering Your Study Registering your study is a simple, three-step process: Request access to register your study. Create a CEDAR account, if you do not already have one. Register your study on the HEAL Data Platform. After completing registration, the next step will be completing your metadata form in CEDAR. How to request access Overview \u00b6 The HEAL Data Platform provides a single interface where users can browse and search for all HEAL-funded studies as well as other HEAL-relevant datasets. In this way, the Platform plays a fundamental role in making HEAL data findable as part of adhering to the FAIR data principles (i.e., data should be Findable, Accessible, Interoperable and Reusable). To achieve this goal, the platform aggregates metadata about each study from multiple sources. One of these is NIH RePORTER from which basic information about all HEAL-funded studies is automatically obtained. However, there are three additional sources of metadata which the Platform can utilize: Information entered into the Center for Expanded Data Annotation and Retrieval (CEDAR) Workbench , an online platform for creating and managing metadata according to the FAIR principles Information from ClinicalTrials.gov for those clinical studies registered there Information from the data repository to which the study is submitting its data To permit the Platform to pull information from these sources, a study must be registered . Registration is the process of linking a study to one or more of the metadata/data sources listed above. This process can be performed by the Principal Investigator or another member of the research team. What if I'm not ready to submit data yet? You may register your study at any time, even if you are not yet ready to submit data to a repository. Registering your study increases its visibility on the Platform and allows the Platform and data repository to anticipate and plan for your subsequent data submission.","title":"Study Registration"},{"location":"study-registration/#study-registration","text":"Registering Your Study Registering your study is a simple, three-step process: Request access to register your study. Create a CEDAR account, if you do not already have one. Register your study on the HEAL Data Platform. After completing registration, the next step will be completing your metadata form in CEDAR. How to request access","title":"Study Registration"},{"location":"study-registration/#overview","text":"The HEAL Data Platform provides a single interface where users can browse and search for all HEAL-funded studies as well as other HEAL-relevant datasets. In this way, the Platform plays a fundamental role in making HEAL data findable as part of adhering to the FAIR data principles (i.e., data should be Findable, Accessible, Interoperable and Reusable). To achieve this goal, the platform aggregates metadata about each study from multiple sources. One of these is NIH RePORTER from which basic information about all HEAL-funded studies is automatically obtained. However, there are three additional sources of metadata which the Platform can utilize: Information entered into the Center for Expanded Data Annotation and Retrieval (CEDAR) Workbench , an online platform for creating and managing metadata according to the FAIR principles Information from ClinicalTrials.gov for those clinical studies registered there Information from the data repository to which the study is submitting its data To permit the Platform to pull information from these sources, a study must be registered . Registration is the process of linking a study to one or more of the metadata/data sources listed above. This process can be performed by the Principal Investigator or another member of the research team. What if I'm not ready to submit data yet? You may register your study at any time, even if you are not yet ready to submit data to a repository. Registering your study increases its visibility on the Platform and allows the Platform and data repository to anticipate and plan for your subsequent data submission.","title":"Overview"},{"location":"study-registration/registering-your-study/","text":"Registering Your Study \u00b6 Info You must request access before you can register your study. Also, if you have not already done so, create an account with CEDAR (choose the \"Register\" option to quickly set up an account). You will need a CEDAR account to complete the registration process. Note that your CEDAR User UUID can be found at the top of your CEDAR profile page. Info This video highlights the next step in registering your study on the HEAL Data Platform. Registration with the HEAL Platform is necessary for compliance with the HEAL Data Sharing Policy. Written instructions are below. Upon receiving notification that you\u2019ve been granted access to register your study, please proceed with the steps outlined below. Step 1: Login to the HEAL Data Platform \u00b6 Step 2: Find your study \u00b6 From the Discovery Page , find the study you wish to register: Click on the study to open the Study Page At the top of the Study Page, select \u2018Register This Study\u2019 to navigate to the Study Registration form . Step 3: Complete the Study Registration form \u00b6 The Study field will already be filled in If known, a valid ClinicalTrials.gov ID (NCT #) can be entered, which will enable the platform to pull some additional metadata related to the study. If known, select the repository you have submitted data to, or intend to submit data to. This will help the Platform us and the repository to which you have submitted or will be submitting your data to anticipate and plan accordingly. If your repository is not listed, please contact us with those details. Enter the unique ID for your study within the repository. Submit your registration. After you successfully register your study, the study will disappear from the HEAL Data Platform for about 24 hours while we sync the study record with the new CEDAR form. You do not need to do anything for it to reappear. Alternate steps to register a study: Login to the Study Registration form Choose the study you wish to register from the Study dropdown. Only those studies you have been approved to register will be displayed in the dropdown. Complete the form as noted above and submit. Next Steps: Fill out your CEDAR form \u00b6 The act of submitting the study registration form will result in the creation of a metadata input form within your CEDAR account: Find the form for your study under the 'Shared with Me' folder on CEDAR (accessible in the left-hand navigation). When you enter additional data into the CEDAR form, be sure to SAVE your changes by scrolling to the bottom of the form. The HEAL Data Platform will pull entries from the CEDAR templates into the Platform to enhance search capabilities and provide increasingly robust study details. An overview of completing the CEDAR metadata form can be found here .","title":"Registering Your Study"},{"location":"study-registration/registering-your-study/#registering-your-study","text":"Info You must request access before you can register your study. Also, if you have not already done so, create an account with CEDAR (choose the \"Register\" option to quickly set up an account). You will need a CEDAR account to complete the registration process. Note that your CEDAR User UUID can be found at the top of your CEDAR profile page. Info This video highlights the next step in registering your study on the HEAL Data Platform. Registration with the HEAL Platform is necessary for compliance with the HEAL Data Sharing Policy. Written instructions are below. Upon receiving notification that you\u2019ve been granted access to register your study, please proceed with the steps outlined below.","title":"Registering Your Study"},{"location":"study-registration/registering-your-study/#step-1-login-to-the-heal-data-platform","text":"","title":"Step 1:  Login to the HEAL Data Platform"},{"location":"study-registration/registering-your-study/#step-2-find-your-study","text":"From the Discovery Page , find the study you wish to register: Click on the study to open the Study Page At the top of the Study Page, select \u2018Register This Study\u2019 to navigate to the Study Registration form .","title":"Step 2: Find your study"},{"location":"study-registration/registering-your-study/#step-3-complete-the-study-registration-form","text":"The Study field will already be filled in If known, a valid ClinicalTrials.gov ID (NCT #) can be entered, which will enable the platform to pull some additional metadata related to the study. If known, select the repository you have submitted data to, or intend to submit data to. This will help the Platform us and the repository to which you have submitted or will be submitting your data to anticipate and plan accordingly. If your repository is not listed, please contact us with those details. Enter the unique ID for your study within the repository. Submit your registration. After you successfully register your study, the study will disappear from the HEAL Data Platform for about 24 hours while we sync the study record with the new CEDAR form. You do not need to do anything for it to reappear. Alternate steps to register a study: Login to the Study Registration form Choose the study you wish to register from the Study dropdown. Only those studies you have been approved to register will be displayed in the dropdown. Complete the form as noted above and submit.","title":"Step 3:  Complete the Study Registration form"},{"location":"study-registration/registering-your-study/#next-steps-fill-out-your-cedar-form","text":"The act of submitting the study registration form will result in the creation of a metadata input form within your CEDAR account: Find the form for your study under the 'Shared with Me' folder on CEDAR (accessible in the left-hand navigation). When you enter additional data into the CEDAR form, be sure to SAVE your changes by scrolling to the bottom of the form. The HEAL Data Platform will pull entries from the CEDAR templates into the Platform to enhance search capabilities and provide increasingly robust study details. An overview of completing the CEDAR metadata form can be found here .","title":"Next Steps: Fill out your CEDAR form"},{"location":"study-registration/requesting-access/","text":"Requesting Access \u00b6 Info This video highlights the first step of registering your study on the HEAL Data Platform. Registration with the HEAL Platform is necessary for compliance with the HEAL Data Sharing Policy. Written instructions are below. Step 1: Login to the HEAL Data Platform \u00b6 Step 2: Find your study \u00b6 From the Discovery Page , find the study you wish to request access to register. (If you cannot find your study, contact us at heal-support@gen3.org.) Click on the study to open the Study Page At the top of the Study Page, select Request Access to Register This Study to navigate to the Study Registration Access Request form. In cases where a study is represented by multiple records on the HEAL Data Platform but data and/or other digital artifacts will only be shared collectively (e.g., through a DCC or in the case of a study that has records from multiple years of funding and/or supplements but will be sharing data only through the parent award), it's likely not necessary that each grant be represented and registered as a stand-alone record on the Platform. If you would like to discuss the representation of your awards on the HEAL Data Platform, please contact us at heal-support@gen3.org . Step 3: Complete the Study Registration Access Request Form \u00b6 The field Study Name - Grant Number will already be filled in. You will need to provide your name, your email address, institutional affiliation and role on the project/study. After submitting, you will receive an email indicating the status of your request within one business day. When approved, you will then be able to register your study .","title":"Requesting Access"},{"location":"study-registration/requesting-access/#requesting-access","text":"Info This video highlights the first step of registering your study on the HEAL Data Platform. Registration with the HEAL Platform is necessary for compliance with the HEAL Data Sharing Policy. Written instructions are below.","title":"Requesting Access"},{"location":"study-registration/requesting-access/#step-1-login-to-the-heal-data-platform","text":"","title":"Step 1: Login to the HEAL Data Platform"},{"location":"study-registration/requesting-access/#step-2-find-your-study","text":"From the Discovery Page , find the study you wish to request access to register. (If you cannot find your study, contact us at heal-support@gen3.org.) Click on the study to open the Study Page At the top of the Study Page, select Request Access to Register This Study to navigate to the Study Registration Access Request form. In cases where a study is represented by multiple records on the HEAL Data Platform but data and/or other digital artifacts will only be shared collectively (e.g., through a DCC or in the case of a study that has records from multiple years of funding and/or supplements but will be sharing data only through the parent award), it's likely not necessary that each grant be represented and registered as a stand-alone record on the Platform. If you would like to discuss the representation of your awards on the HEAL Data Platform, please contact us at heal-support@gen3.org .","title":"Step 2: Find your study"},{"location":"study-registration/requesting-access/#step-3-complete-the-study-registration-access-request-form","text":"The field Study Name - Grant Number will already be filled in. You will need to provide your name, your email address, institutional affiliation and role on the project/study. After submitting, you will receive an email indicating the status of your request within one business day. When approved, you will then be able to register your study .","title":"Step 3: Complete the Study Registration Access Request Form"},{"location":"vlmd/","text":"Variable-level Metadata Submission \u00b6 The HEAL Data Platform contains variable-level metadata (VLMD) from two sources: Data Dictionaries: these files describe each variable within a study's dataset HEAL CDEs (Common Data Elements): these are validated and structured questionnaires used by investigators conducting research with human participants to harmonize the data collected across studies Info In order to submit any VLMD to the Platform, two conditions must be true: 1) The study must already be registered: If your study is not yet registered on the HEAL Data Platform, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study 2) You must have access to submit VLMD: If you are the person who registered your study, you automatically have access. If someone else registered your study, you can follow the instructions below to request access to submit VLMD. How to Request Access to Submit VLMD Data Dictionaries \u00b6 Data Dictionaries, which contain variable-level metadata, describe each variable within a dataset. Examples of variable-level attributes include the variable name, description (or variable label), type, format, terminology, and source. In order to submit a study\u2019s data dictionary to the Platform, the data dictionary must conform to the HEAL variable-level metadata schema . The following instructions will demonstrate how to use the HEAL Data Utilities' VLMD tool to help you generate a HEAL-compliant data dictionary from your dataset or existing data dictionary, and then submit it to the Platform. How to Generate a HEAL-compliant Data Dictionary How to Submit a HEAL-compliant Data Dictionary HEAL CDEs (Common Data Elements) \u00b6 HEAL CDEs are validated and structured questionnaires that include fields describing the human subject data collected, along with how the data was gathered, and how the response is represented in the dataset. HEAL investigators conducting research with human participants use CDEs to allow the data collected by different studies to be harmonized for further analyses. (To learn more about HEAL CDEs, visit this NIH page.) To report the CDEs your study used to the Platform, follow the instructions on this page .","title":"Variable-level Metadata Submission"},{"location":"vlmd/#variable-level-metadata-submission","text":"The HEAL Data Platform contains variable-level metadata (VLMD) from two sources: Data Dictionaries: these files describe each variable within a study's dataset HEAL CDEs (Common Data Elements): these are validated and structured questionnaires used by investigators conducting research with human participants to harmonize the data collected across studies Info In order to submit any VLMD to the Platform, two conditions must be true: 1) The study must already be registered: If your study is not yet registered on the HEAL Data Platform, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study 2) You must have access to submit VLMD: If you are the person who registered your study, you automatically have access. If someone else registered your study, you can follow the instructions below to request access to submit VLMD. How to Request Access to Submit VLMD","title":"Variable-level Metadata Submission"},{"location":"vlmd/#data-dictionaries","text":"Data Dictionaries, which contain variable-level metadata, describe each variable within a dataset. Examples of variable-level attributes include the variable name, description (or variable label), type, format, terminology, and source. In order to submit a study\u2019s data dictionary to the Platform, the data dictionary must conform to the HEAL variable-level metadata schema . The following instructions will demonstrate how to use the HEAL Data Utilities' VLMD tool to help you generate a HEAL-compliant data dictionary from your dataset or existing data dictionary, and then submit it to the Platform. How to Generate a HEAL-compliant Data Dictionary How to Submit a HEAL-compliant Data Dictionary","title":"Data Dictionaries"},{"location":"vlmd/#heal-cdes-common-data-elements","text":"HEAL CDEs are validated and structured questionnaires that include fields describing the human subject data collected, along with how the data was gathered, and how the response is represented in the dataset. HEAL investigators conducting research with human participants use CDEs to allow the data collected by different studies to be harmonized for further analyses. (To learn more about HEAL CDEs, visit this NIH page.) To report the CDEs your study used to the Platform, follow the instructions on this page .","title":"HEAL CDEs (Common Data Elements)"},{"location":"vlmd/vlmd_healdata_utils/","text":"Generate a HEAL-compliant Data Dictionary \u00b6 Info The following instructions pertain to the stand-alone, executable version of the HEAL VLMD tool as well as the use of the VLMD tool in HEAL Workspaces. These two options are recommended for users who are unfamiliar with installing Python software and/or who want to generate VLMD documents in the quickest and easiest way possible. If you would like to install and integrate the VLMD tool into an existing, local pipeline, please see the HEAL Data Utilities on GitHub or PyPi for more information. The HEAL VLMD tool was created to help investigators generate HEAL-compliant variable-level metadata (VLMD) documents that may be uploaded to the HEAL Data Platform. This VLMD tool uses a command-line interface (CLI), is available within HEAL Data Platform Workspaces, and can be incorporated into existing pipelines in the form of a Python module. Using the Stand-alone VLMD Tool \u00b6 In an effort to further streamline the VLMD extraction process for researchers, we have developed a stand-alone executable version of the VLMD tool. Download the VLMD Tool You can download the latest version of the VLMD tool for your operating system (i.e., MacOS, Windows, Linux) from the NIH HEAL Initiative\u2019s GitHub repository: Download Latest Software Release Once you have downloaded the appropriate zip file, double-click the file to unzip the package. You should then see a file labeled vlmd or vlmd.exe , depending on your operating system and how it is configured. Double-clicking vlmd will then open your computer's command-line interface (CLI). Once the interface opens and the VLMD tool is loaded, you will be presented with four prompts: documentation, extract, start, and validate. CLI Commands \u00b6 extract \u00b6 Extract the variable level metadata from an existing file with a specific type/format start \u00b6 Start a data dictionary from an empty template validate \u00b6 Check (validate) an existing HEAL Data Dictionary file to see if it follows the HEAL specifications after filling out a template or further annotation after extracting from a different format. Info Typing the documentation command will launch the VLMD Data Dictionary definitions in the HEAL Data Utilities documentation . Using the VLMD Tool in HEAL Workspaces with Python \u00b6 The VLMD tool has also been preloaded into a HEAL workspace, so that you may use it there instead of downloading it to your local machine. To request access to a workspace, see instructions here . Once workspace access has been approved, select the (Generic) Jupyter Lab Notebook with R Kernel to get started using the VLMD tool. You can start by uploading your REDCap data dictionary or data file to the persistent drive (/pd). Any data not saved to the persistent drive will be lost when the workspace is terminated. For more information, please see our documentation on HEAL workspaces . Info Files containing human subjects data must be de-identified before uploading them to a workspace , and the user is responsible for ensuring that he or she has permission to upload the data to the cloud. Workspaces are secure and any file(s) a user uploads are only accessible by that user. If you are extracting variable-level metadata from a dataset stored in a format that contains metadata (e.g., Stata, SAS or SPSS), our recommendation is to make a copy of the dataset in which all of the data (i.e., the actual observations) have been deleted, leaving only the variable names, formats, labels, etc. Many people are unaware that this is possible, but it makes a great way of sharing information about your dataset without sharing the data themselves. And it is easy to do. For example, in Stata, once you have a dataset loaded in memory all that is required is: drop in 1/l save empty_dataset Similarily, in SAS: data empty_dataset; set original_dataset; stop; run; where the stop statement stops SAS from processing any rows. In either case, this will leave you with an empty dataset containing all of the original variable-level metadata which you may safely upload to a workspace for use with the VLMD tool. After you\u2019ve launched the workspace and uploaded your data dictionary or data file, you can import the necessary functions. Below are examples of how to extract VLMD from an SPSS data file, create a new VLMD file from scratch, and validate an existing data dictionary in CSV and JSON formats, all within a workspace. Python Functions \u00b6 extract from healdata_utils import convert_to_vlmd convert_to_vlmd ( input_filepath = \"~/pd/myfile.sav\" , inputtype = \"spss\" ) Note Currently the python subcommand is convert_to_vlmd but will be changed to extract_to_vlmd to be consistent with CLI. extract was chosen to better reflect the functionality. start from healdata_utils import write_vlmd_template write_vlmd_template ( tmpdir . joinpath ( \"heal.csv\" ), numfields = 10 ) validate from healdata_utils import validate_vlmd_csv , validate_vlmd_json validate_vlmd_csv ( \"data/myhealcsvdd.csv\" ) validate_vlmd_json ( \"data/myhealjsondd.json\" ) Input \u00b6 There are many applications and software packages that are commonly used during the data collection and processing phases of studies. The HEAL VLMD tool accommodates several of these different input file formats. Please follow the links below if you would like to learn more: CSV datasets CSV (minimal) data dictionary SPSS datasets SAS datasets Stata datasets REDCap data dictionary Frictionless Table Schema Excel dataset Output \u00b6 VLMD extraction will result in a JSON and CSV version of the HEAL data dictionary in the output folder along with the validation reports in the errors folder. See below: Errors \u00b6 heal-csv-errors.json \u00b6 outputted validation report for table in csv file against frictionless schema If valid, this file will contain: { \"valid\" : true , \"errors\" : [] } heal-json-errors.json \u00b6 outputted jsonschema validation report. If valid, this file will contain: { \"valid\" : true , \"errors\" : [] } If no outputdir specified, the resulting HEAL-compliant data dictionaries will be named: heal-csvtemplate-data-dictionary.csv : This is the CSV data dictionary heal-jsontemplate-data-dictionary.json : This is the JSON version of the data dictionary For more information on workflows, functions, and definitions, please see the HEAL Data Utilities Documentation . Workflow Summary \u00b6 Typical workflows for creating a HEAL-compliant data dictionary include: Create your data dictionary (a) Run the vlmd extract command (or convert_to_vlmd if in python) to generate a HEAL-compliant data dictionary via your desired input format (b) Run the vlmd template command to start from an empty template. Add/annotate with additional information in your preferred HEAL data dictionary format (either json or csv ). To further annotate and use the data dictionary, see the variable-level metadata field property information below: csv data dictionary json data dictionary Run the vlmd validate command with your HEAL data dictioanry as the input to validate. Repeat (2) and (3) until you are ready to submit. Please note, currently only name and description are required. Next Steps \u00b6 Once you\u2019ve created your HEAL-compliant data dictionary, you\u2019re now ready to submit your data dictionary to the Platform. Please see our instructions on submitting a data dictionary. How to Submit a Data Dictionary If you have need any help generating a HEAL-compliant data dictionary with the VMLD Tool, or have a general inquiry, please contact us at heal-support@gen3.org","title":"Generate a HEAL-compliant Data Dictionary"},{"location":"vlmd/vlmd_healdata_utils/#generate-a-heal-compliant-data-dictionary","text":"Info The following instructions pertain to the stand-alone, executable version of the HEAL VLMD tool as well as the use of the VLMD tool in HEAL Workspaces. These two options are recommended for users who are unfamiliar with installing Python software and/or who want to generate VLMD documents in the quickest and easiest way possible. If you would like to install and integrate the VLMD tool into an existing, local pipeline, please see the HEAL Data Utilities on GitHub or PyPi for more information. The HEAL VLMD tool was created to help investigators generate HEAL-compliant variable-level metadata (VLMD) documents that may be uploaded to the HEAL Data Platform. This VLMD tool uses a command-line interface (CLI), is available within HEAL Data Platform Workspaces, and can be incorporated into existing pipelines in the form of a Python module.","title":"Generate a HEAL-compliant Data Dictionary"},{"location":"vlmd/vlmd_healdata_utils/#using-the-stand-alone-vlmd-tool","text":"In an effort to further streamline the VLMD extraction process for researchers, we have developed a stand-alone executable version of the VLMD tool. Download the VLMD Tool You can download the latest version of the VLMD tool for your operating system (i.e., MacOS, Windows, Linux) from the NIH HEAL Initiative\u2019s GitHub repository: Download Latest Software Release Once you have downloaded the appropriate zip file, double-click the file to unzip the package. You should then see a file labeled vlmd or vlmd.exe , depending on your operating system and how it is configured. Double-clicking vlmd will then open your computer's command-line interface (CLI). Once the interface opens and the VLMD tool is loaded, you will be presented with four prompts: documentation, extract, start, and validate.","title":"Using the Stand-alone VLMD Tool"},{"location":"vlmd/vlmd_healdata_utils/#cli-commands","text":"","title":"CLI Commands"},{"location":"vlmd/vlmd_healdata_utils/#extract","text":"Extract the variable level metadata from an existing file with a specific type/format","title":"extract"},{"location":"vlmd/vlmd_healdata_utils/#start","text":"Start a data dictionary from an empty template","title":"start"},{"location":"vlmd/vlmd_healdata_utils/#validate","text":"Check (validate) an existing HEAL Data Dictionary file to see if it follows the HEAL specifications after filling out a template or further annotation after extracting from a different format. Info Typing the documentation command will launch the VLMD Data Dictionary definitions in the HEAL Data Utilities documentation .","title":"validate"},{"location":"vlmd/vlmd_healdata_utils/#using-the-vlmd-tool-in-heal-workspaces-with-python","text":"The VLMD tool has also been preloaded into a HEAL workspace, so that you may use it there instead of downloading it to your local machine. To request access to a workspace, see instructions here . Once workspace access has been approved, select the (Generic) Jupyter Lab Notebook with R Kernel to get started using the VLMD tool. You can start by uploading your REDCap data dictionary or data file to the persistent drive (/pd). Any data not saved to the persistent drive will be lost when the workspace is terminated. For more information, please see our documentation on HEAL workspaces . Info Files containing human subjects data must be de-identified before uploading them to a workspace , and the user is responsible for ensuring that he or she has permission to upload the data to the cloud. Workspaces are secure and any file(s) a user uploads are only accessible by that user. If you are extracting variable-level metadata from a dataset stored in a format that contains metadata (e.g., Stata, SAS or SPSS), our recommendation is to make a copy of the dataset in which all of the data (i.e., the actual observations) have been deleted, leaving only the variable names, formats, labels, etc. Many people are unaware that this is possible, but it makes a great way of sharing information about your dataset without sharing the data themselves. And it is easy to do. For example, in Stata, once you have a dataset loaded in memory all that is required is: drop in 1/l save empty_dataset Similarily, in SAS: data empty_dataset; set original_dataset; stop; run; where the stop statement stops SAS from processing any rows. In either case, this will leave you with an empty dataset containing all of the original variable-level metadata which you may safely upload to a workspace for use with the VLMD tool. After you\u2019ve launched the workspace and uploaded your data dictionary or data file, you can import the necessary functions. Below are examples of how to extract VLMD from an SPSS data file, create a new VLMD file from scratch, and validate an existing data dictionary in CSV and JSON formats, all within a workspace.","title":"Using the VLMD Tool in HEAL Workspaces with Python"},{"location":"vlmd/vlmd_healdata_utils/#python-functions","text":"extract from healdata_utils import convert_to_vlmd convert_to_vlmd ( input_filepath = \"~/pd/myfile.sav\" , inputtype = \"spss\" ) Note Currently the python subcommand is convert_to_vlmd but will be changed to extract_to_vlmd to be consistent with CLI. extract was chosen to better reflect the functionality. start from healdata_utils import write_vlmd_template write_vlmd_template ( tmpdir . joinpath ( \"heal.csv\" ), numfields = 10 ) validate from healdata_utils import validate_vlmd_csv , validate_vlmd_json validate_vlmd_csv ( \"data/myhealcsvdd.csv\" ) validate_vlmd_json ( \"data/myhealjsondd.json\" )","title":"Python Functions"},{"location":"vlmd/vlmd_healdata_utils/#input","text":"There are many applications and software packages that are commonly used during the data collection and processing phases of studies. The HEAL VLMD tool accommodates several of these different input file formats. Please follow the links below if you would like to learn more: CSV datasets CSV (minimal) data dictionary SPSS datasets SAS datasets Stata datasets REDCap data dictionary Frictionless Table Schema Excel dataset","title":"Input"},{"location":"vlmd/vlmd_healdata_utils/#output","text":"VLMD extraction will result in a JSON and CSV version of the HEAL data dictionary in the output folder along with the validation reports in the errors folder. See below:","title":"Output"},{"location":"vlmd/vlmd_healdata_utils/#errors","text":"","title":"Errors"},{"location":"vlmd/vlmd_healdata_utils/#heal-csv-errorsjson","text":"outputted validation report for table in csv file against frictionless schema If valid, this file will contain: { \"valid\" : true , \"errors\" : [] }","title":"heal-csv-errors.json"},{"location":"vlmd/vlmd_healdata_utils/#heal-json-errorsjson","text":"outputted jsonschema validation report. If valid, this file will contain: { \"valid\" : true , \"errors\" : [] } If no outputdir specified, the resulting HEAL-compliant data dictionaries will be named: heal-csvtemplate-data-dictionary.csv : This is the CSV data dictionary heal-jsontemplate-data-dictionary.json : This is the JSON version of the data dictionary For more information on workflows, functions, and definitions, please see the HEAL Data Utilities Documentation .","title":"heal-json-errors.json"},{"location":"vlmd/vlmd_healdata_utils/#workflow-summary","text":"Typical workflows for creating a HEAL-compliant data dictionary include: Create your data dictionary (a) Run the vlmd extract command (or convert_to_vlmd if in python) to generate a HEAL-compliant data dictionary via your desired input format (b) Run the vlmd template command to start from an empty template. Add/annotate with additional information in your preferred HEAL data dictionary format (either json or csv ). To further annotate and use the data dictionary, see the variable-level metadata field property information below: csv data dictionary json data dictionary Run the vlmd validate command with your HEAL data dictioanry as the input to validate. Repeat (2) and (3) until you are ready to submit. Please note, currently only name and description are required.","title":"Workflow Summary"},{"location":"vlmd/vlmd_healdata_utils/#next-steps","text":"Once you\u2019ve created your HEAL-compliant data dictionary, you\u2019re now ready to submit your data dictionary to the Platform. Please see our instructions on submitting a data dictionary. How to Submit a Data Dictionary If you have need any help generating a HEAL-compliant data dictionary with the VMLD Tool, or have a general inquiry, please contact us at heal-support@gen3.org","title":"Next Steps"},{"location":"vlmd/vlmd_request_access/","text":"Request Access to Submit Variable-level Metadata (VLMD) \u00b6 Info In order to submit a Data Dictionary or report CDEs to the Platform, the study must already be registered on the HEAL Data Platform. If your study is not yet registered, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study Request Access to Submit Variable-level Metadata \u00b6 Requesting Access is the first step in submitting VLMD to the HEAL Data Platform. If you were the person that registered the study on the Platform, you can go directly to Submit VLMD . If someone else registered the study, follow the steps below to request access to submit VLMD. Login to the HEAL Data Platform \u00b6 Click on the Login button in the upper right corner to log in to the Platform. Find your study \u00b6 From the Discovery Page , find the study for which you wish to request access to submit VLMD. Click on the study to open the Study Page At the top of the Study Page, select Request Access to Submit Variable-level Metadata to navigate to the VLMD Submission Request Form. Info If you do not see Request Access to Submit Variable-level Metadata , see below for how to proceed. If you see: Login to... - you are not logged in. Log in first , then repeat the instructions above. Request Access to Register This Study - your study is not yet registered on the HEAL Data Platform. Follow these instructions to request access and register your study . Submit Variable-level Metadata - You already have access to submit VLMD. Proceed to submit your data dictionary or CDEs . Complete the VLMD Submission Request Form \u00b6 The field Study Name - Grant Number will already be filled in. You will need to provide your name, your email address, institutional affiliation and role on the project/study. After submitting, you will see a message indicating it was successfully sent. Within one business day, you will receive an email reporting that your request has been approved. When approved, you will then be able to submit VLMD (either a data dictionary or CDEs) to the Platform. Submit VLMD \u00b6 You are now ready to submit VLMD! Find instructions for submitting a Data Dictionary or reporting CDEs below: Generate and Submit a HEAL-compliant Data Dictionary Report CDEs","title":"Request Access to Submit VLMD"},{"location":"vlmd/vlmd_request_access/#request-access-to-submit-variable-level-metadata-vlmd","text":"Info In order to submit a Data Dictionary or report CDEs to the Platform, the study must already be registered on the HEAL Data Platform. If your study is not yet registered, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study","title":"Request Access to Submit Variable-level Metadata (VLMD)"},{"location":"vlmd/vlmd_request_access/#request-access-to-submit-variable-level-metadata","text":"Requesting Access is the first step in submitting VLMD to the HEAL Data Platform. If you were the person that registered the study on the Platform, you can go directly to Submit VLMD . If someone else registered the study, follow the steps below to request access to submit VLMD.","title":"Request Access to Submit Variable-level Metadata"},{"location":"vlmd/vlmd_request_access/#login-to-the-heal-data-platform","text":"Click on the Login button in the upper right corner to log in to the Platform.","title":"Login to the HEAL Data Platform"},{"location":"vlmd/vlmd_request_access/#find-your-study","text":"From the Discovery Page , find the study for which you wish to request access to submit VLMD. Click on the study to open the Study Page At the top of the Study Page, select Request Access to Submit Variable-level Metadata to navigate to the VLMD Submission Request Form. Info If you do not see Request Access to Submit Variable-level Metadata , see below for how to proceed. If you see: Login to... - you are not logged in. Log in first , then repeat the instructions above. Request Access to Register This Study - your study is not yet registered on the HEAL Data Platform. Follow these instructions to request access and register your study . Submit Variable-level Metadata - You already have access to submit VLMD. Proceed to submit your data dictionary or CDEs .","title":"Find your study"},{"location":"vlmd/vlmd_request_access/#complete-the-vlmd-submission-request-form","text":"The field Study Name - Grant Number will already be filled in. You will need to provide your name, your email address, institutional affiliation and role on the project/study. After submitting, you will see a message indicating it was successfully sent. Within one business day, you will receive an email reporting that your request has been approved. When approved, you will then be able to submit VLMD (either a data dictionary or CDEs) to the Platform.","title":"Complete the VLMD Submission Request Form"},{"location":"vlmd/vlmd_request_access/#submit-vlmd","text":"You are now ready to submit VLMD! Find instructions for submitting a Data Dictionary or reporting CDEs below: Generate and Submit a HEAL-compliant Data Dictionary Report CDEs","title":"Submit VLMD"},{"location":"vlmd/vlmd_submission/","text":"Submit a Data Dictionary \u00b6 Info In order to submit a Data Dictionary to the Platform, the following conditions must be true: 1) The study must already be registered: If your study is not yet registered on the HEAL Data Platform, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study 2) You must have access to submit VLMD: If you are the person who registered your study, you automatically have access. If someone else registered your study, you can follow the instructions below to request access to submit VLMD. How to Request Access to Submit VLMD 3) The Data Dictionary must conform to the HEAL VLMD schema: See below for the instructions for how to generate a HEAL-compliant data dictionary. Generate a HEAL-compliant Data Dictionary 4) Submissions should not include data of any kind . Submissions containing data will be removed from the Platform. More information and submission templates can be found here . Login to the HEAL Data Platform \u00b6 Click on the Login button in the upper right corner to log in to the Platform. Find Your Study \u00b6 From the Discovery Page , find the study for which you wish to submit a data dictionary. Click on the study to open the Study Page At the top of the Study Page, select Submit Variable-level Metadata to navigate to the Variable-level Metadata Submission Form . Info If you do not see Submit Variable-level Metadata , see below for how to proceed. If you see: Login to... - you are not logged in. Log in first , then repeat the instructions above. Request Access to Submit Variable-level Metadata - you have not yet gotten access to submit VLMD. Follow these instructions to request access to submit VLMD . Request Access to Register This Study - your study is not yet registered on the HEAL Data Platform. Follow these instructions to request access and register your study . Submit Your Data Dictionary \u00b6 The VLMD submission form defaults to the Data Dictionary tab (see box at top). The Study field will already be filled in Choose the Select File button to browse your local computer for your data dictionary. Only TSV, CSV and JSON files can be submitted While multiple dictionaries can be associated with a study, the process currently supports one submission at a time. Please repeat this process for each data dictionary you wish to submit Submissions should not include data of any kind. Submissions containing data will be removed from the Platform. Enter a name for your data dictionary. This name will be visible to users on the Platform. Previously-submitted data dictionary names are displayed, if applicable; using the same name for a new submission will overwrite the existing record. To facilitate processing of your submission, some administrative information is needed to allow HEAL Data Platform staff to contact you should the need arise: First and Last name E-mail address Note that this information is not stored on the Platform, but is simply needed to support you throughout the submission process. Submit your data dictionary Processing Your Submission \u00b6 Upon receipt of your successful submission, HEAL Data Platform staff will: Review your submission for compliance with the HEAL Data Platform variable-level metadata schema Ensure no data are included. Files containing study-generated data of any kind will not be accepted. HEAL Data Platform staff will contact you with any questions and/or to work through any issues that may arise. If there are no issues, you will be notified when processing is completed. Additional Help \u00b6 More information about the HEAL variable-level metadata schema, as well as submission templates, can be found here . If you have issues with a submission, or have a general inquiry, please contact us at heal-support@gen3.org .","title":"Submit a Data Dictionary"},{"location":"vlmd/vlmd_submission/#submit-a-data-dictionary","text":"Info In order to submit a Data Dictionary to the Platform, the following conditions must be true: 1) The study must already be registered: If your study is not yet registered on the HEAL Data Platform, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study 2) You must have access to submit VLMD: If you are the person who registered your study, you automatically have access. If someone else registered your study, you can follow the instructions below to request access to submit VLMD. How to Request Access to Submit VLMD 3) The Data Dictionary must conform to the HEAL VLMD schema: See below for the instructions for how to generate a HEAL-compliant data dictionary. Generate a HEAL-compliant Data Dictionary 4) Submissions should not include data of any kind . Submissions containing data will be removed from the Platform. More information and submission templates can be found here .","title":"Submit a Data Dictionary"},{"location":"vlmd/vlmd_submission/#login-to-the-heal-data-platform","text":"Click on the Login button in the upper right corner to log in to the Platform.","title":"Login to the HEAL Data Platform"},{"location":"vlmd/vlmd_submission/#find-your-study","text":"From the Discovery Page , find the study for which you wish to submit a data dictionary. Click on the study to open the Study Page At the top of the Study Page, select Submit Variable-level Metadata to navigate to the Variable-level Metadata Submission Form . Info If you do not see Submit Variable-level Metadata , see below for how to proceed. If you see: Login to... - you are not logged in. Log in first , then repeat the instructions above. Request Access to Submit Variable-level Metadata - you have not yet gotten access to submit VLMD. Follow these instructions to request access to submit VLMD . Request Access to Register This Study - your study is not yet registered on the HEAL Data Platform. Follow these instructions to request access and register your study .","title":"Find Your Study"},{"location":"vlmd/vlmd_submission/#submit-your-data-dictionary","text":"The VLMD submission form defaults to the Data Dictionary tab (see box at top). The Study field will already be filled in Choose the Select File button to browse your local computer for your data dictionary. Only TSV, CSV and JSON files can be submitted While multiple dictionaries can be associated with a study, the process currently supports one submission at a time. Please repeat this process for each data dictionary you wish to submit Submissions should not include data of any kind. Submissions containing data will be removed from the Platform. Enter a name for your data dictionary. This name will be visible to users on the Platform. Previously-submitted data dictionary names are displayed, if applicable; using the same name for a new submission will overwrite the existing record. To facilitate processing of your submission, some administrative information is needed to allow HEAL Data Platform staff to contact you should the need arise: First and Last name E-mail address Note that this information is not stored on the Platform, but is simply needed to support you throughout the submission process. Submit your data dictionary","title":"Submit Your Data Dictionary"},{"location":"vlmd/vlmd_submission/#processing-your-submission","text":"Upon receipt of your successful submission, HEAL Data Platform staff will: Review your submission for compliance with the HEAL Data Platform variable-level metadata schema Ensure no data are included. Files containing study-generated data of any kind will not be accepted. HEAL Data Platform staff will contact you with any questions and/or to work through any issues that may arise. If there are no issues, you will be notified when processing is completed.","title":"Processing Your Submission"},{"location":"vlmd/vlmd_submission/#additional-help","text":"More information about the HEAL variable-level metadata schema, as well as submission templates, can be found here . If you have issues with a submission, or have a general inquiry, please contact us at heal-support@gen3.org .","title":"Additional Help"},{"location":"vlmd/vlmd_submit_CDE/","text":"Submit HEAL CDEs \u00b6 Info In order to report your CDEs to the Platform, the following conditions must be true: 1) The study must already be registered: If your study is not yet registered on the HEAL Data Platform, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study 2) You must have access to submit VLMD: If you are the person who registered your study, you automatically have access. If someone else registered your study, you can follow the instructions below to request access to submit VLMD. How to Request Access to Submit VLMD Login to the HEAL Data Platform \u00b6 Click on the Login button in the upper right corner to log in to the Platform. Find Your Study \u00b6 From the Discovery Page , find the study for which you wish to report CDEs. Click on the study to open the Study Page At the top of the Study Page, select Submit Variable-level Metadata to navigate to the Variable-level Metadata Submission Form . Info If you do not see Submit Variable-level Metadata , see below for how to proceed. If you see: Login to... - you are not logged in. Log in first , then repeat the instructions above. Request Access to Submit Variable-level Metadata - you have not yet gotten access to submit VLMD. Follow these instructions to request access to submit VLMD . Request Access to Register This Study - your study is not yet registered on the HEAL Data Platform. Follow these instructions to request access and register your study . Report CDEs \u00b6 The VLMD submission form defaults to the Data Dictionary tab, so click the CDE tab (see box at top) to navigate to the CDE submission form. The Study field will already be filled in. Below the Study field is a checklist of core CDEs. Check the box for any you wish to associate with your study. As you select CDEs, they will populate in the box at the bottom of the section. If you or someone else on your team has already submitted CDEs, you will see these checked and listed in the box. You can update the CDE list by adding or removing CDEs and resubmitting the form. To add Supplemental CDEs not in the core list, you can click in the box beneath the core list and begin typing CDE names or codes to filter other CDEs that include the typed string (see the yellow highlighting). Click the CDE in the filtered list to add. To facilitate processing your submission, some administrative information is needed to allow HEAL Data Platform staff to contact you if necessary: First and Last name E-mail address Note that this information is not stored on the Platform, but is simply needed to support you throughout the submission process. Submit your CDEs by clicking the Submit CDEs button. Viewing the CDEs on Your Study Record \u00b6 The CDEs will be updated on your study record on the Platform within 24 hours of your submission. You will then be able to see the CDEs listed in the facets under the study name. Additional Help \u00b6 If you have issues with a submission, or have a general inquiry, please contact us at heal-support@gen3.org .","title":"Submit HEAL CDEs"},{"location":"vlmd/vlmd_submit_CDE/#submit-heal-cdes","text":"Info In order to report your CDEs to the Platform, the following conditions must be true: 1) The study must already be registered: If your study is not yet registered on the HEAL Data Platform, please see our instructions for how to register your study before submitting VLMD. How to Register Your Study 2) You must have access to submit VLMD: If you are the person who registered your study, you automatically have access. If someone else registered your study, you can follow the instructions below to request access to submit VLMD. How to Request Access to Submit VLMD","title":"Submit HEAL CDEs"},{"location":"vlmd/vlmd_submit_CDE/#login-to-the-heal-data-platform","text":"Click on the Login button in the upper right corner to log in to the Platform.","title":"Login to the HEAL Data Platform"},{"location":"vlmd/vlmd_submit_CDE/#find-your-study","text":"From the Discovery Page , find the study for which you wish to report CDEs. Click on the study to open the Study Page At the top of the Study Page, select Submit Variable-level Metadata to navigate to the Variable-level Metadata Submission Form . Info If you do not see Submit Variable-level Metadata , see below for how to proceed. If you see: Login to... - you are not logged in. Log in first , then repeat the instructions above. Request Access to Submit Variable-level Metadata - you have not yet gotten access to submit VLMD. Follow these instructions to request access to submit VLMD . Request Access to Register This Study - your study is not yet registered on the HEAL Data Platform. Follow these instructions to request access and register your study .","title":"Find Your Study"},{"location":"vlmd/vlmd_submit_CDE/#report-cdes","text":"The VLMD submission form defaults to the Data Dictionary tab, so click the CDE tab (see box at top) to navigate to the CDE submission form. The Study field will already be filled in. Below the Study field is a checklist of core CDEs. Check the box for any you wish to associate with your study. As you select CDEs, they will populate in the box at the bottom of the section. If you or someone else on your team has already submitted CDEs, you will see these checked and listed in the box. You can update the CDE list by adding or removing CDEs and resubmitting the form. To add Supplemental CDEs not in the core list, you can click in the box beneath the core list and begin typing CDE names or codes to filter other CDEs that include the typed string (see the yellow highlighting). Click the CDE in the filtered list to add. To facilitate processing your submission, some administrative information is needed to allow HEAL Data Platform staff to contact you if necessary: First and Last name E-mail address Note that this information is not stored on the Platform, but is simply needed to support you throughout the submission process. Submit your CDEs by clicking the Submit CDEs button.","title":"Report CDEs"},{"location":"vlmd/vlmd_submit_CDE/#viewing-the-cdes-on-your-study-record","text":"The CDEs will be updated on your study record on the Platform within 24 hours of your submission. You will then be able to see the CDEs listed in the facets under the study name.","title":"Viewing the CDEs on Your Study Record"},{"location":"vlmd/vlmd_submit_CDE/#additional-help","text":"If you have issues with a submission, or have a general inquiry, please contact us at heal-support@gen3.org .","title":"Additional Help"},{"location":"vlmd/schemas/csv-fields/","text":"HEAL Variable Level Metadata Fields \u00b6 Variable level metadata individual fields integrated into the variable level metadata object within the HEAL platform metadata service. NOTE Only name and description properties are required. For categorical variables, constraints.enum and encodings (where applicable) properties are highly encouraged. For studies using HEAL or other common data elements (CDEs), standardsMappings information is highly encouraged. type and format properties may be particularly useful for some variable types (e.g. date-like variables) Properties \u00b6 module (string) The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: Demographics PROMIS Substance use Medical History Sleep questions Physical activity name (string,required) The name of a variable (i.e., field) as it appears in the data. title (string) The human-readable title or label of the variable. Examples: My Variable Gender identity description (string,required) An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Examples: The participant's age at the time of study enrollment What is the highest grade or level of school you have completed or the highest degree you have received? type (string) A classification or category of a particular data element or property expected or allowed in the dataset. Definitions: number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \\\"test\\\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \\\"2023-05-25\\\")) datetime (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) time (A specific time of day. (e.g., \\\"10:30:00\\\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \\\"2023-05\\\")) duration (A length of time. (e.g., \\\"PT1H\\\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) Possible values: number integer string any boolean date datetime time year yearmonth duration geopoint format (string) Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats . If type is \"date\", \"datetime\", or \"time\", default format is ISO8601 formatting for those respective types (see details on ISO8601 format for Date , Datetime , or Time ) - If you want to specify a date-like variable using standard Python/C strptime syntax, see here for details. See here for more information about appropriate format values by variable type . [Additional information] Date Formats (date, datetime, time type variable): A format for a date variable ( date , time , datetime ). default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. {PATTERN} : The value can be parsed according to {PATTERN} , which MUST follow the date formatting syntax of C / Python strftime such as: %Y-%m-%d (for date, e.g., 2023-05-25) %Y%-%d (for date, e.g., 20230525) for date without dashes %Y-%m-%dT%H:%M:%S (for datetime, e.g., 2023-05-25T10:30:45) %Y-%m-%dT%H:%M:%SZ (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z) %Y-%m-%dT%H:%M:%S%z (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300) %Y-%m-%dT%H:%M (for datetime without seconds, e.g., 2023-05-25T10:30) %Y-%m-%dT%H (for datetime without minutes and seconds, e.g., 2023-05-25T10) %H:%M:%S (for time, e.g., 10:30:45) %H:%M:%SZ (for time with UTC timezone, e.g., 10:30:45Z) %H:%M:%S%z (for time with timezone offset, e.g., 10:30:45+0300) String formats: email if valid emails (e.g., test@gmail.com) uri if valid uri addresses (e.g., https://example.com/resource123) binary if a base64 binary encoded string (e.g., authentication token like aGVsbG8gd29ybGQ=) uuid if a universal unique identifier also known as a guid (eg., f47ac10b-58cc-4372-a567-0e02b2c3d479) Geopoint formats: The two types of formats for geopoint (describing a geographic point). array (if 'lat,long' (e.g., 36.63,-90.20)) object (if {'lat':36.63,'lon':-90.20}) constraints.maxLength (integer) Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. constraints.enum (string) Constrains possible values to a set of values. Examples: 1|2|3|4|5|6|7|8 White|Black or African American|American Indian or Alaska Native|Native Hawaiian or Other Pacific Islander|Asian|Some other race|Multiracial constraints.pattern (string) A regular expression pattern the data MUST conform to. constraints.maximum (integer) Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. constraints.minimum (integer) Specifies the minimum value of a field. encodings (string) Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). Examples: 0=No|1=Yes HW=Hello world|GBW=Good bye world|HM=Hi,Mike ordered (boolean) Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). missingValues (string) A list of missing values specific to a variable. Examples: Missing|Skipped|No preference Missing trueValues (string) For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Examples: Required|REQUIRED required|Yes|Y|Checked Checked Required falseValues (string) For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) A link to the variable as it exists on the home repository, if applicable standardsMappings.url (string) The url that links out to the published, standardized mapping. Examples: https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI standardsMappings.type (string) The type of mapping linked to a published set of standard variables such as the NIH Common Data Elements program Examples: cde ontology reference_list standardsMappings.label (string) A free text label of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program. Examples: substance use chemical compound promis standardsMappings.source (string) The source of the standardized variable. Examples: TBD (will have controlled vocabulary) standardsMappings.id (string) The id locating the individual mapping within the given source. relatedConcepts.url (string) The url that links out to the published, standardized concept. Examples: https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI relatedConcepts.type (string) The type of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) relatedConcepts.label (string) A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) relatedConcepts.source (string) The source of the related concept. Examples: TBD (will have controlled vocabulary) relatedConcepts.id (string) The id locating the individual mapping within the given source. univarStats.median (number) univarStats.mean (number) univarStats.std (number) univarStats.min (number) univarStats.max (number) univarStats.mode (number) univarStats.count (integer) univarStats.twentyFifthPercentile (number) univarStats.seventyFifthPercentile (number) univarStats.categoricalMarginals.name (string) univarStats.categoricalMarginals.count (integer)","title":"HEAL Variable Level Metadata Fields"},{"location":"vlmd/schemas/csv-fields/#heal-variable-level-metadata-fields","text":"Variable level metadata individual fields integrated into the variable level metadata object within the HEAL platform metadata service. NOTE Only name and description properties are required. For categorical variables, constraints.enum and encodings (where applicable) properties are highly encouraged. For studies using HEAL or other common data elements (CDEs), standardsMappings information is highly encouraged. type and format properties may be particularly useful for some variable types (e.g. date-like variables)","title":"HEAL Variable Level Metadata Fields"},{"location":"vlmd/schemas/csv-fields/#properties","text":"module (string) The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: Demographics PROMIS Substance use Medical History Sleep questions Physical activity name (string,required) The name of a variable (i.e., field) as it appears in the data. title (string) The human-readable title or label of the variable. Examples: My Variable Gender identity description (string,required) An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Examples: The participant's age at the time of study enrollment What is the highest grade or level of school you have completed or the highest degree you have received? type (string) A classification or category of a particular data element or property expected or allowed in the dataset. Definitions: number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \\\"test\\\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \\\"2023-05-25\\\")) datetime (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) time (A specific time of day. (e.g., \\\"10:30:00\\\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \\\"2023-05\\\")) duration (A length of time. (e.g., \\\"PT1H\\\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) Possible values: number integer string any boolean date datetime time year yearmonth duration geopoint format (string) Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats . If type is \"date\", \"datetime\", or \"time\", default format is ISO8601 formatting for those respective types (see details on ISO8601 format for Date , Datetime , or Time ) - If you want to specify a date-like variable using standard Python/C strptime syntax, see here for details. See here for more information about appropriate format values by variable type . [Additional information] Date Formats (date, datetime, time type variable): A format for a date variable ( date , time , datetime ). default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. {PATTERN} : The value can be parsed according to {PATTERN} , which MUST follow the date formatting syntax of C / Python strftime such as: %Y-%m-%d (for date, e.g., 2023-05-25) %Y%-%d (for date, e.g., 20230525) for date without dashes %Y-%m-%dT%H:%M:%S (for datetime, e.g., 2023-05-25T10:30:45) %Y-%m-%dT%H:%M:%SZ (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z) %Y-%m-%dT%H:%M:%S%z (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300) %Y-%m-%dT%H:%M (for datetime without seconds, e.g., 2023-05-25T10:30) %Y-%m-%dT%H (for datetime without minutes and seconds, e.g., 2023-05-25T10) %H:%M:%S (for time, e.g., 10:30:45) %H:%M:%SZ (for time with UTC timezone, e.g., 10:30:45Z) %H:%M:%S%z (for time with timezone offset, e.g., 10:30:45+0300) String formats: email if valid emails (e.g., test@gmail.com) uri if valid uri addresses (e.g., https://example.com/resource123) binary if a base64 binary encoded string (e.g., authentication token like aGVsbG8gd29ybGQ=) uuid if a universal unique identifier also known as a guid (eg., f47ac10b-58cc-4372-a567-0e02b2c3d479) Geopoint formats: The two types of formats for geopoint (describing a geographic point). array (if 'lat,long' (e.g., 36.63,-90.20)) object (if {'lat':36.63,'lon':-90.20}) constraints.maxLength (integer) Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. constraints.enum (string) Constrains possible values to a set of values. Examples: 1|2|3|4|5|6|7|8 White|Black or African American|American Indian or Alaska Native|Native Hawaiian or Other Pacific Islander|Asian|Some other race|Multiracial constraints.pattern (string) A regular expression pattern the data MUST conform to. constraints.maximum (integer) Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. constraints.minimum (integer) Specifies the minimum value of a field. encodings (string) Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). Examples: 0=No|1=Yes HW=Hello world|GBW=Good bye world|HM=Hi,Mike ordered (boolean) Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). missingValues (string) A list of missing values specific to a variable. Examples: Missing|Skipped|No preference Missing trueValues (string) For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Examples: Required|REQUIRED required|Yes|Y|Checked Checked Required falseValues (string) For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) A link to the variable as it exists on the home repository, if applicable standardsMappings.url (string) The url that links out to the published, standardized mapping. Examples: https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI standardsMappings.type (string) The type of mapping linked to a published set of standard variables such as the NIH Common Data Elements program Examples: cde ontology reference_list standardsMappings.label (string) A free text label of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program. Examples: substance use chemical compound promis standardsMappings.source (string) The source of the standardized variable. Examples: TBD (will have controlled vocabulary) standardsMappings.id (string) The id locating the individual mapping within the given source. relatedConcepts.url (string) The url that links out to the published, standardized concept. Examples: https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI relatedConcepts.type (string) The type of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) relatedConcepts.label (string) A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) relatedConcepts.source (string) The source of the related concept. Examples: TBD (will have controlled vocabulary) relatedConcepts.id (string) The id locating the individual mapping within the given source. univarStats.median (number) univarStats.mean (number) univarStats.std (number) univarStats.min (number) univarStats.max (number) univarStats.mode (number) univarStats.count (integer) univarStats.twentyFifthPercentile (number) univarStats.seventyFifthPercentile (number) univarStats.categoricalMarginals.name (string) univarStats.categoricalMarginals.count (integer)","title":"Properties"},{"location":"vlmd/schemas/json-data-dictionary/","text":"Variable Level Metadata (Data Dictionaries) \u00b6 This schema defines the variable level metadata for one data dictionary for a given study.Note a given study can have multiple data dictionaries title (string,required) \u00b6 description (string) \u00b6 data_dictionary (array,required) \u00b6 Variable level metadata individual fields integrated into the variable level metadata object within the HEAL platform metadata service. NOTE Only name and description properties are required. For categorical variables, constraints.enum and encodings (where applicable) properties are highly encouraged. For studies using HEAL or other common data elements (CDEs), standardsMappings information is highly encouraged. type and format properties may be particularly useful for some variable types (e.g. date-like variables) Properties for each record \u00b6 module (string) The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: Demographics PROMIS Substance use Medical History Sleep questions Physical activity name (string,required) The name of a variable (i.e., field) as it appears in the data. title (string) The human-readable title or label of the variable. Examples: My Variable Gender identity description (string,required) An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Examples: The participant's age at the time of study enrollment What is the highest grade or level of school you have completed or the highest degree you have received? type (string) A classification or category of a particular data element or property expected or allowed in the dataset. Definitions: number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \"test\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \"2023-05-25\")) datetime (A specific date and time, including timezone information. (e.g., \"2023-05-25T10:30:00Z\")) time (A specific time of day. (e.g., \"10:30:00\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \"2023-05\")) duration (A length of time. (e.g., \"PT1H\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) Possible values: number integer string any boolean date datetime time year yearmonth duration geopoint format (string) Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats . If type is \"date\", \"datetime\", or \"time\", default format is ISO8601 formatting for those respective types (see details on ISO8601 format for Date , Datetime , or Time ) - If you want to specify a date-like variable using standard Python/C strptime syntax, see here for details. See here for more information about appropriate format values by variable type . [Additional information] Date Formats (date, datetime, time type variable): A format for a date variable ( date , time , datetime ). default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. {PATTERN} : The value can be parsed according to {PATTERN} , which MUST follow the date formatting syntax of C / Python strftime such as: %Y-%m-%d (for date, e.g., 2023-05-25) %Y%-%d (for date, e.g., 20230525) for date without dashes %Y-%m-%dT%H:%M:%S (for datetime, e.g., 2023-05-25T10:30:45) %Y-%m-%dT%H:%M:%SZ (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z) %Y-%m-%dT%H:%M:%S%z (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300) %Y-%m-%dT%H:%M (for datetime without seconds, e.g., 2023-05-25T10:30) %Y-%m-%dT%H (for datetime without minutes and seconds, e.g., 2023-05-25T10) %H:%M:%S (for time, e.g., 10:30:45) %H:%M:%SZ (for time with UTC timezone, e.g., 10:30:45Z) %H:%M:%S%z (for time with timezone offset, e.g., 10:30:45+0300) String formats: email if valid emails (e.g., test@gmail.com) uri if valid uri addresses (e.g., https://example.com/resource123) binary if a base64 binary encoded string (e.g., authentication token like aGVsbG8gd29ybGQ=) uuid if a universal unique identifier also known as a guid (eg., f47ac10b-58cc-4372-a567-0e02b2c3d479) Geopoint formats: The two types of formats for geopoint (describing a geographic point). array (if 'lat,long' (e.g., 36.63,-90.20)) object (if {'lat':36.63,'lon':-90.20}) constraints (object) maxLength (integer) Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. enum (array) Constrains possible values to a set of values. Examples: [ 1 , 2 , 3 , 4 ] [ 'Whi te ' , 'Black or A fr ica n America n ' , 'America n I n dia n or Alaska Na t ive' , 'Na t ive Hawaiia n or O t her Paci f ic Isla n der' , 'Asia n ' , 'Some o t her race' , 'Mul t iracial' ] pattern (string) A regular expression pattern the data MUST conform to. maximum (integer) Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. minimum (integer) Specifies the minimum value of a field. encodings (object) Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). Examples: { ' 0 ' : 'No' , ' 1 ' : 'Yes' } { 'HW' : 'Hello world' , 'GBW' : 'Good bye world' , 'HM' : 'Hi , Mike' } ordered (boolean) Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). missingValues (array) A list of missing values specific to a variable. Examples: [ 'Missi n g' , 'Skipped' , 'No pre feren ce' ] [ 'Missi n g' ] trueValues (array) For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Examples: [ 'required' , 'Yes' , 'Checked' ] [ 'required' ] falseValues (array) For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) A link to the variable as it exists on the home repository, if applicable standardsMappings (array) A published set of standard variables such as the NIH Common Data Elements program. relatedConcepts (array) Mappings to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) univarStats (object) Univariate statistics inferred from the data about the given variable median (number) mean (number) std (number) min (number) max (number) mode (number) count (integer) twentyFifthPercentile (number) seventyFifthPercentile (number) categoricalMarginals (array)","title":"Variable Level Metadata (Data Dictionaries)"},{"location":"vlmd/schemas/json-data-dictionary/#variable-level-metadata-data-dictionaries","text":"This schema defines the variable level metadata for one data dictionary for a given study.Note a given study can have multiple data dictionaries","title":"Variable Level Metadata (Data Dictionaries)"},{"location":"vlmd/schemas/json-data-dictionary/#title-stringrequired","text":"","title":"title (string,required)"},{"location":"vlmd/schemas/json-data-dictionary/#description-string","text":"","title":"description (string)"},{"location":"vlmd/schemas/json-data-dictionary/#data_dictionary-arrayrequired","text":"Variable level metadata individual fields integrated into the variable level metadata object within the HEAL platform metadata service. NOTE Only name and description properties are required. For categorical variables, constraints.enum and encodings (where applicable) properties are highly encouraged. For studies using HEAL or other common data elements (CDEs), standardsMappings information is highly encouraged. type and format properties may be particularly useful for some variable types (e.g. date-like variables)","title":"data_dictionary (array,required)"},{"location":"vlmd/schemas/json-data-dictionary/#properties-for-each-record","text":"module (string) The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: Demographics PROMIS Substance use Medical History Sleep questions Physical activity name (string,required) The name of a variable (i.e., field) as it appears in the data. title (string) The human-readable title or label of the variable. Examples: My Variable Gender identity description (string,required) An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Examples: The participant's age at the time of study enrollment What is the highest grade or level of school you have completed or the highest degree you have received? type (string) A classification or category of a particular data element or property expected or allowed in the dataset. Definitions: number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \"test\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \"2023-05-25\")) datetime (A specific date and time, including timezone information. (e.g., \"2023-05-25T10:30:00Z\")) time (A specific time of day. (e.g., \"10:30:00\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \"2023-05\")) duration (A length of time. (e.g., \"PT1H\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) Possible values: number integer string any boolean date datetime time year yearmonth duration geopoint format (string) Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats . If type is \"date\", \"datetime\", or \"time\", default format is ISO8601 formatting for those respective types (see details on ISO8601 format for Date , Datetime , or Time ) - If you want to specify a date-like variable using standard Python/C strptime syntax, see here for details. See here for more information about appropriate format values by variable type . [Additional information] Date Formats (date, datetime, time type variable): A format for a date variable ( date , time , datetime ). default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. {PATTERN} : The value can be parsed according to {PATTERN} , which MUST follow the date formatting syntax of C / Python strftime such as: %Y-%m-%d (for date, e.g., 2023-05-25) %Y%-%d (for date, e.g., 20230525) for date without dashes %Y-%m-%dT%H:%M:%S (for datetime, e.g., 2023-05-25T10:30:45) %Y-%m-%dT%H:%M:%SZ (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z) %Y-%m-%dT%H:%M:%S%z (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300) %Y-%m-%dT%H:%M (for datetime without seconds, e.g., 2023-05-25T10:30) %Y-%m-%dT%H (for datetime without minutes and seconds, e.g., 2023-05-25T10) %H:%M:%S (for time, e.g., 10:30:45) %H:%M:%SZ (for time with UTC timezone, e.g., 10:30:45Z) %H:%M:%S%z (for time with timezone offset, e.g., 10:30:45+0300) String formats: email if valid emails (e.g., test@gmail.com) uri if valid uri addresses (e.g., https://example.com/resource123) binary if a base64 binary encoded string (e.g., authentication token like aGVsbG8gd29ybGQ=) uuid if a universal unique identifier also known as a guid (eg., f47ac10b-58cc-4372-a567-0e02b2c3d479) Geopoint formats: The two types of formats for geopoint (describing a geographic point). array (if 'lat,long' (e.g., 36.63,-90.20)) object (if {'lat':36.63,'lon':-90.20}) constraints (object) maxLength (integer) Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. enum (array) Constrains possible values to a set of values. Examples: [ 1 , 2 , 3 , 4 ] [ 'Whi te ' , 'Black or A fr ica n America n ' , 'America n I n dia n or Alaska Na t ive' , 'Na t ive Hawaiia n or O t her Paci f ic Isla n der' , 'Asia n ' , 'Some o t her race' , 'Mul t iracial' ] pattern (string) A regular expression pattern the data MUST conform to. maximum (integer) Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. minimum (integer) Specifies the minimum value of a field. encodings (object) Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). Examples: { ' 0 ' : 'No' , ' 1 ' : 'Yes' } { 'HW' : 'Hello world' , 'GBW' : 'Good bye world' , 'HM' : 'Hi , Mike' } ordered (boolean) Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). missingValues (array) A list of missing values specific to a variable. Examples: [ 'Missi n g' , 'Skipped' , 'No pre feren ce' ] [ 'Missi n g' ] trueValues (array) For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Examples: [ 'required' , 'Yes' , 'Checked' ] [ 'required' ] falseValues (array) For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) A link to the variable as it exists on the home repository, if applicable standardsMappings (array) A published set of standard variables such as the NIH Common Data Elements program. relatedConcepts (array) Mappings to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) univarStats (object) Univariate statistics inferred from the data about the given variable median (number) mean (number) std (number) min (number) max (number) mode (number) count (integer) twentyFifthPercentile (number) seventyFifthPercentile (number) categoricalMarginals (array)","title":"Properties for each record"},{"location":"workspaces/","text":"Workspace User Guide \u00b6 Workspaces are secure data analysis environments in the cloud that can access data from one or more data resources in the HEAL Data Ecosystem. By default, workspaces on the HEAL Data Platform include Jupyter notebooks, Python and R, but can be configured to host virtually any application, including analysis workflows, data processing pipelines, or data visualization tools. New to Jupyter? Learn more about the popular tool for data scientists on Jupyter.org (disclaimer: CTDS is not responsible for the content). Register for Workspace Access on the HEAL Data Platform Getting Started with Workspaces on the HEAL Data Platform","title":"Workspace User Guide"},{"location":"workspaces/#workspace-user-guide","text":"Workspaces are secure data analysis environments in the cloud that can access data from one or more data resources in the HEAL Data Ecosystem. By default, workspaces on the HEAL Data Platform include Jupyter notebooks, Python and R, but can be configured to host virtually any application, including analysis workflows, data processing pipelines, or data visualization tools. New to Jupyter? Learn more about the popular tool for data scientists on Jupyter.org (disclaimer: CTDS is not responsible for the content). Register for Workspace Access on the HEAL Data Platform Getting Started with Workspaces on the HEAL Data Platform","title":"Workspace User Guide"},{"location":"workspaces/heal_workspace_registration/","text":"Register for HEAL Data Platform Workspaces \u00b6 To start exploring workspaces on the HEAL Data Platform, users can apply for Temporary Trial Access. Extended access to workspaces on the HEAL Data Platform is granted using an NIH STRIDES workspace account, which can be requested after trial access is provisioned. Please see below for more details. Guidelines for Requesting Temporary Trial Access to HEAL Data Platform Workspaces \u00b6 For new users without workspace access, please follow these steps: Login to the HEAL Data Platform . Please make a note of your specific login username; this is the username that will have access to workspaces. Click on the Workspace tab . This opens the Workspace Access Request form Fill in the details and submit the form shown below. The form should be completed only once. Following submission, users will see a success message and a link back to the Discovery page. If you see any other message besides a success message, please reach out to us at heal-support@gen3.org . We may not have received your access request. Users will receive another email notifying them the temporary trial access request has been approved. They should then be able to access workspaces on the HEAL Data Platform. Please note that the timeline for this approval can be a few business days. If you have not received a response after a few business days, please reach out to heal-support@gen3.org to check the status of your access request.* Guidelines for Requesting Extended Access to HEAL Data Platform Workspaces using STRIDES \u00b6 Please Note: The process for granting access for a workspace account through NIH STRIDES can take up to two weeks. The workspace account can be funded through NIH STRIDES (NIH S cience and T echnology R esearch I nfrastructure for D iscovery, E xperimentation, and S ustainability). The NIH STRIDES Initiative allows NIH-funded researchers to explore the use of cloud environments to streamline NIH data use by partnering with commercial providers. By leveraging the STRIDES Initiative, NIH and NIH-funded institutions can begin to create a robust, interconnected ecosystem that breaks down silos related to generating, analyzing, and sharing research data. NIH-funded researchers with an active NIH award may take advantage of the STRIDES Initiative for their NIH-funded research projects. Eligible researchers include NIH intramural researchers and awardees of NIH contracts, other transaction agreements, grants, cooperative agreements, and other agreements. More information on NIH STRIDES and how to gain access can be found here . Please see below for registration steps Users will receive an invitation via email to register for an NIH STRIDES workspace account. Users can click the link in the invitation email or request a workspace account by visiting https://healportal.org/ and logging in. After authorization, users will be able to see active workspace accounts and credits. To request a workspace account, select \"Request New Workspace\" on the landing page. Choose one of the two options a) STRIDES Grant/Award Funded or b) STRIDES Credits to request a workspace account. For information on the NIH STRIDES options, please refer to the official page . The STRIDES Grant/Award Funded form can be selected if researchers have received NIH funding (e.g. a grant, contract, cooperative agreement, or other transaction agreement) and intend to use these funds for a HEAL Data Platform Workspace account. With this option, the researchers' organization will be responsible for payment. Select the STRIDES Credits form to request credits from the NIH STRIDES Initiative for the HEAL Data Platform Workspace account. With this option, once the request is approved, a new account with a spending limit of $XXX will be provisioned for usage. Submit the request. Note that the process of granting access for a workspace account can take up to two weeks and users will be notified. Following the approval, users will see the current workspace accounts and credits on the landing page.","title":"Register for HEAL Data Platform Workspaces"},{"location":"workspaces/heal_workspace_registration/#register-for-heal-data-platform-workspaces","text":"To start exploring workspaces on the HEAL Data Platform, users can apply for Temporary Trial Access. Extended access to workspaces on the HEAL Data Platform is granted using an NIH STRIDES workspace account, which can be requested after trial access is provisioned. Please see below for more details.","title":"Register for HEAL Data Platform Workspaces"},{"location":"workspaces/heal_workspace_registration/#guidelines-for-requesting-temporary-trial-access-to-heal-data-platform-workspaces","text":"For new users without workspace access, please follow these steps: Login to the HEAL Data Platform . Please make a note of your specific login username; this is the username that will have access to workspaces. Click on the Workspace tab . This opens the Workspace Access Request form Fill in the details and submit the form shown below. The form should be completed only once. Following submission, users will see a success message and a link back to the Discovery page. If you see any other message besides a success message, please reach out to us at heal-support@gen3.org . We may not have received your access request. Users will receive another email notifying them the temporary trial access request has been approved. They should then be able to access workspaces on the HEAL Data Platform. Please note that the timeline for this approval can be a few business days. If you have not received a response after a few business days, please reach out to heal-support@gen3.org to check the status of your access request.*","title":"Guidelines for Requesting Temporary Trial Access to HEAL Data Platform Workspaces"},{"location":"workspaces/heal_workspace_registration/#guidelines-for-requesting-extended-access-to-heal-data-platform-workspaces-using-strides","text":"Please Note: The process for granting access for a workspace account through NIH STRIDES can take up to two weeks. The workspace account can be funded through NIH STRIDES (NIH S cience and T echnology R esearch I nfrastructure for D iscovery, E xperimentation, and S ustainability). The NIH STRIDES Initiative allows NIH-funded researchers to explore the use of cloud environments to streamline NIH data use by partnering with commercial providers. By leveraging the STRIDES Initiative, NIH and NIH-funded institutions can begin to create a robust, interconnected ecosystem that breaks down silos related to generating, analyzing, and sharing research data. NIH-funded researchers with an active NIH award may take advantage of the STRIDES Initiative for their NIH-funded research projects. Eligible researchers include NIH intramural researchers and awardees of NIH contracts, other transaction agreements, grants, cooperative agreements, and other agreements. More information on NIH STRIDES and how to gain access can be found here . Please see below for registration steps Users will receive an invitation via email to register for an NIH STRIDES workspace account. Users can click the link in the invitation email or request a workspace account by visiting https://healportal.org/ and logging in. After authorization, users will be able to see active workspace accounts and credits. To request a workspace account, select \"Request New Workspace\" on the landing page. Choose one of the two options a) STRIDES Grant/Award Funded or b) STRIDES Credits to request a workspace account. For information on the NIH STRIDES options, please refer to the official page . The STRIDES Grant/Award Funded form can be selected if researchers have received NIH funding (e.g. a grant, contract, cooperative agreement, or other transaction agreement) and intend to use these funds for a HEAL Data Platform Workspace account. With this option, the researchers' organization will be responsible for payment. Select the STRIDES Credits form to request credits from the NIH STRIDES Initiative for the HEAL Data Platform Workspace account. With this option, once the request is approved, a new account with a spending limit of $XXX will be provisioned for usage. Submit the request. Note that the process of granting access for a workspace account can take up to two weeks and users will be notified. Following the approval, users will see the current workspace accounts and credits on the landing page.","title":"Guidelines for Requesting Extended Access to HEAL Data Platform Workspaces using STRIDES"},{"location":"workspaces/heal_workspaces/","text":"Using Workspaces on the HEAL Data Platform \u00b6 Info To use workspaces, you must first register for workspace access as described on the Workspace Registration page . Guidelines to get started in Workspaces \u00b6 Once you have access to workspaces, use this guide below to get started with analysis work in workspaces. Log in via https://healdata.org/portal/login to access workspaces. After navigating to https://healdata.org/portal/workspace , you will discover a list of pre-configured virtual machine (VM) images, as shown below. (Generic) Jupyter Notebook with R kernel: Choose this VM if you are familiar with setting up Python- or R-based Notebooks, or if you just exported one or multiple studies from the Discovery Page and want to start your custom analysis. Tutorial Notebooks: Explore our Jupyter Notebook tutorials written in Python or R, which analyze data pulled from various sources on the HEAL Data Platform These are excellent resources for code to use to analyze data from HEAL, and examples that illustrate the variety of data and analyses available through HEAL. Click \u201cLaunch\u201d on any of the workspace options to spin up a copy of that VM. The status of launching the workspace is displayed after clicking on \u201cLaunch\u201d. Note: Launching the VM may take several minutes. After launching, the home folder is displayed. One of these folders is your persistent drive (\"/pd\"). Select the /pd folder. New files or licenses should be saved in the the /pd directory if users need to access them after restarting the workspaces. Only files saved in the /pd directory will remain available after termination of a workspace session. Attention: Any personal files in the folder \u201cdata\u201d will be lost. Personal files in the directory /pd will persist. Do not save files in the \"data\" or \u201cdata/healdata.org\u201d folders. The folder \u201chealdata.org\u201d in the \u201cdata\u201d folder will host the data files you have exported from the Discovery Page. Move these files to the /pd directory if you do not want to have to export them again. /pd has a capacity limit of 10GB. Start a new notebook under \u201cNotebook\u201d in the Launcher tab. Click the tiles in the launcher and choose between Python 3 or R Studio as the base programming language. Note: You can open and run multiple notebooks in your workspace; however, the generic, tutorial and nextflow workspace images are currently separate Docker images. There is no functionality to combine them or run nextflow in the Tutorial or Generic images. This may be available in the future, after further testing and development activities. Experiment away! Code blocks are entered in cells, which can be executed individually or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. Do not forget to terminate your workspace when you are done with your session. Unterminated workspaces can continue to accrue computational costs. Note: workspaces automatically shut down after 90 minutes of idle time . Further reading: read more about how to download data files into the Workspaces here . Upload, Save, and Download Files/Notebooks \u00b6 You can upload data files or Notebooks from your local machine to the home directory by clicking on \u201cUpload\u201d in the top left corner. Access the uploaded content in the Notebook (see below). You can then save the notebook by clicking \"File\" - \"Save as\", as shown below. You can then download notebooks by clicking \"File\" - \"Download\", as shown below. Download the notebook, for example, as \".ipynb\". Environments, Languages, and Tools \u00b6 The following environments are available in the workspaces: Jupyter Lab The following programming languages are available in Jupyter Notebooks: R Python 3 The following tools are available in Jupyter Notebooks: GitHub ( read GitHub documentation ) Python 3 and R in Jupyter \u00b6 Both Python 3 and R are available in Jupyter Notebooks. Basic Python or R packages , such as PyPI or CRAN, as well as many tools typical for data analysis are already included in the base workspace images without further installation required. For Python and R, users can start a new notebook with one of the tiles under \"Notebook\", as shown below. Automatic Workspace Shutdown \u00b6 Warning: When a HEAL Workspace reaches the STRIDES Credits limit for STRIDES Credits Workspaces, or reaches the Hard Limit for STRIDES Grant Workspaces, the Workspace will be automatically terminated. Please be sure to save any work before reaching the STRIDES Credit or Hard Limit. Warning: Workspaces will also automatically shut down after 90 minutes of idle time. A pop-up window will remind users to navigate back to the workspaces page in order to save the data.","title":"Using Workspaces on the HEAL Data Platform"},{"location":"workspaces/heal_workspaces/#using-workspaces-on-the-heal-data-platform","text":"Info To use workspaces, you must first register for workspace access as described on the Workspace Registration page .","title":"Using Workspaces on the HEAL Data Platform"},{"location":"workspaces/heal_workspaces/#guidelines-to-get-started-in-workspaces","text":"Once you have access to workspaces, use this guide below to get started with analysis work in workspaces. Log in via https://healdata.org/portal/login to access workspaces. After navigating to https://healdata.org/portal/workspace , you will discover a list of pre-configured virtual machine (VM) images, as shown below. (Generic) Jupyter Notebook with R kernel: Choose this VM if you are familiar with setting up Python- or R-based Notebooks, or if you just exported one or multiple studies from the Discovery Page and want to start your custom analysis. Tutorial Notebooks: Explore our Jupyter Notebook tutorials written in Python or R, which analyze data pulled from various sources on the HEAL Data Platform These are excellent resources for code to use to analyze data from HEAL, and examples that illustrate the variety of data and analyses available through HEAL. Click \u201cLaunch\u201d on any of the workspace options to spin up a copy of that VM. The status of launching the workspace is displayed after clicking on \u201cLaunch\u201d. Note: Launching the VM may take several minutes. After launching, the home folder is displayed. One of these folders is your persistent drive (\"/pd\"). Select the /pd folder. New files or licenses should be saved in the the /pd directory if users need to access them after restarting the workspaces. Only files saved in the /pd directory will remain available after termination of a workspace session. Attention: Any personal files in the folder \u201cdata\u201d will be lost. Personal files in the directory /pd will persist. Do not save files in the \"data\" or \u201cdata/healdata.org\u201d folders. The folder \u201chealdata.org\u201d in the \u201cdata\u201d folder will host the data files you have exported from the Discovery Page. Move these files to the /pd directory if you do not want to have to export them again. /pd has a capacity limit of 10GB. Start a new notebook under \u201cNotebook\u201d in the Launcher tab. Click the tiles in the launcher and choose between Python 3 or R Studio as the base programming language. Note: You can open and run multiple notebooks in your workspace; however, the generic, tutorial and nextflow workspace images are currently separate Docker images. There is no functionality to combine them or run nextflow in the Tutorial or Generic images. This may be available in the future, after further testing and development activities. Experiment away! Code blocks are entered in cells, which can be executed individually or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files. Do not forget to terminate your workspace when you are done with your session. Unterminated workspaces can continue to accrue computational costs. Note: workspaces automatically shut down after 90 minutes of idle time . Further reading: read more about how to download data files into the Workspaces here .","title":"Guidelines to get started in Workspaces"},{"location":"workspaces/heal_workspaces/#upload-save-and-download-filesnotebooks","text":"You can upload data files or Notebooks from your local machine to the home directory by clicking on \u201cUpload\u201d in the top left corner. Access the uploaded content in the Notebook (see below). You can then save the notebook by clicking \"File\" - \"Save as\", as shown below. You can then download notebooks by clicking \"File\" - \"Download\", as shown below. Download the notebook, for example, as \".ipynb\".","title":"Upload, Save, and Download Files/Notebooks"},{"location":"workspaces/heal_workspaces/#environments-languages-and-tools","text":"The following environments are available in the workspaces: Jupyter Lab The following programming languages are available in Jupyter Notebooks: R Python 3 The following tools are available in Jupyter Notebooks: GitHub ( read GitHub documentation )","title":"Environments, Languages, and Tools"},{"location":"workspaces/heal_workspaces/#python-3-and-r-in-jupyter","text":"Both Python 3 and R are available in Jupyter Notebooks. Basic Python or R packages , such as PyPI or CRAN, as well as many tools typical for data analysis are already included in the base workspace images without further installation required. For Python and R, users can start a new notebook with one of the tiles under \"Notebook\", as shown below.","title":"Python 3 and R in Jupyter"},{"location":"workspaces/heal_workspaces/#automatic-workspace-shutdown","text":"Warning: When a HEAL Workspace reaches the STRIDES Credits limit for STRIDES Credits Workspaces, or reaches the Hard Limit for STRIDES Grant Workspaces, the Workspace will be automatically terminated. Please be sure to save any work before reaching the STRIDES Credit or Hard Limit. Warning: Workspaces will also automatically shut down after 90 minutes of idle time. A pop-up window will remind users to navigate back to the workspaces page in order to save the data.","title":"Automatic Workspace Shutdown"}]}